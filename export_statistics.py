#!/usr/bin/env python3
"""
ç»Ÿè®¡ç»“æœå¯¼å‡ºè„šæœ¬
å°†åˆ†ç±»/åˆ†æç»“æœå¯¼å‡ºä¸º JSON/CSV æ ¼å¼ï¼Œä¾¿äºåç»­å¯è§†åŒ–ä½¿ç”¨
"""

import sqlite3
import json
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

class StatisticsExporter:
    """ç»Ÿè®¡ç»“æœå¯¼å‡ºå™¨"""

    def __init__(self, db_path: str = "./data/db.sqlite"):
        self.db_path = db_path
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("./logs")
        log_dir.mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'export_statistics.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('StatisticsExporter')

    def get_customers_by_status(self) -> Dict[str, Any]:
        """æŒ‰å®¢æˆ·çŠ¶æ€ç»Ÿè®¡"""
        self.logger.info("ğŸ‘¥ ç»Ÿè®¡å®¢æˆ·çŠ¶æ€åˆ†å¸ƒ...")

        try:
            conn = sqlite3.connect(self.db_path)

            # åŸºç¡€çŠ¶æ€ç»Ÿè®¡
            query = """
            SELECT
                customer_status,
                COUNT(*) as count,
                COUNT(DISTINCT country) as countries,
                AVG(total_drawings) as avg_drawings,
                MAX(CASE WHEN last_inquiry_date IS NOT NULL THEN last_inquiry_date END) as last_inquiry
            FROM customers
            GROUP BY customer_status
            ORDER BY count DESC
            """

            df = pd.read_sql_query(query, conn)

            # è¯¦ç»†å®¢æˆ·ä¿¡æ¯
            detailed_query = """
            SELECT
                company_name,
                contact_email,
                customer_status,
                customer_level,
                country,
                language,
                total_drawings,
                first_contact_date,
                last_inquiry_date,
                contact_frequency,
                created_at
            FROM customers
            ORDER BY customer_status, total_drawings DESC
            """

            detailed_df = pd.read_sql_query(detailed_query, conn)

            conn.close()

            result = {
                'summary': df.to_dict('records'),
                'detailed': detailed_df.to_dict('records'),
                'total_customers': len(detailed_df),
                'status_distribution': df.set_index('customer_status')['count'].to_dict()
            }

            self.logger.info(f"âœ… å®¢æˆ·çŠ¶æ€ç»Ÿè®¡å®Œæˆ: {len(df)} ä¸ªçŠ¶æ€")
            return result

        except Exception as e:
            self.logger.error(f"âŒ å®¢æˆ·çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_drawings_by_category(self) -> Dict[str, Any]:
        """æŒ‰äº§å“ç±»åˆ«ç»Ÿè®¡å›¾çº¸"""
        self.logger.info("ğŸ“Š ç»Ÿè®¡å›¾çº¸åˆ†ç±»åˆ†å¸ƒ...")

        try:
            conn = sqlite3.connect(self.db_path)

            # åŸºç¡€åˆ†ç±»ç»Ÿè®¡
            query = """
            SELECT
                product_category,
                COUNT(*) as count,
                COUNT(DISTINCT customer_id) as unique_customers,
                AVG(classification_confidence) as avg_confidence,
                COUNT(CASE WHEN standard_or_custom = 1 THEN 1 END) as custom_count,
                COUNT(CASE WHEN standard_or_custom = 0 THEN 1 END) as standard_count,
                MIN(classification_date) as first_classified,
                MAX(classification_date) as last_classified
            FROM drawings
            GROUP BY product_category
            ORDER BY count DESC
            """

            df = pd.read_sql_query(query, conn)

            # æ ‡å‡†ä»¶vså®šåˆ¶ä»¶ç»Ÿè®¡
            standard_custom_query = """
            SELECT
                standard_or_custom,
                COUNT(*) as count,
                COUNT(DISTINCT product_category) as categories
            FROM drawings
            WHERE standard_or_custom IS NOT NULL
            GROUP BY standard_or_custom
            """

            standard_custom_df = pd.read_sql_query(standard_custom_query, conn)

            # æŒ‰æ—¶é—´ç»Ÿè®¡åˆ†ç±»è¶‹åŠ¿
            time_trend_query = """
            SELECT
                DATE(classification_date) as classification_day,
                product_category,
                COUNT(*) as daily_count
            FROM drawings
            WHERE classification_date IS NOT NULL
            GROUP BY DATE(classification_date), product_category
            ORDER BY classification_day DESC
            LIMIT 100
            """

            time_trend_df = pd.read_sql_query(time_trend_query, conn)

            # æŒ‰æ•°æ®æºç»Ÿè®¡
            source_query = """
            SELECT
                data_source,
                COUNT(*) as count,
                COUNT(DISTINCT product_category) as categories
            FROM drawings
            WHERE data_source IS NOT NULL AND data_source != 'unknown'
            GROUP BY data_source
            ORDER BY count DESC
            """

            source_df = pd.read_sql_query(source_query, conn)

            conn.close()

            result = {
                'category_distribution': df.to_dict('records'),
                'standard_vs_custom': standard_custom_df.to_dict('records'),
                'time_trend': time_trend_df.to_dict('records'),
                'data_sources': source_df.to_dict('records'),
                'total_drawings': df['count'].sum(),
                'classified_drawings': df[df['product_category'] != 'æœªåˆ†ç±»']['count'].sum() if len(df) > 0 else 0,
                'classification_rate': (df[df['product_category'] != 'æœªåˆ†ç±»']['count'].sum() / df['count'].sum() * 100) if len(df) > 0 and df['count'].sum() > 0 else 0
            }

            self.logger.info(f"âœ… å›¾çº¸åˆ†ç±»ç»Ÿè®¡å®Œæˆ: {len(df)} ä¸ªç±»åˆ«")
            return result

        except Exception as e:
            self.logger.error(f"âŒ å›¾çº¸åˆ†ç±»ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_factory_performance_stats(self) -> Dict[str, Any]:
        """å·¥å‚è¡¨ç°ç»Ÿè®¡"""
        self.logger.info("ğŸ­ ç»Ÿè®¡å·¥å‚è¡¨ç°...")

        try:
            conn = sqlite3.connect(self.db_path)

            # å·¥å‚æŠ¥ä»·ç»Ÿè®¡
            factory_query = """
            SELECT
                f.id as factory_id,
                f.factory_name,
                f.location,
                f.capability,
                COUNT(fq.id) as total_quotes,
                COUNT(DISTINCT fq.product_category) as unique_categories,
                AVG(fq.price) as avg_price,
                MIN(fq.price) as min_price,
                MAX(fq.price) as max_price,
                AVG(fq.moq) as avg_moq,
                MIN(fq.quote_date) as first_quote,
                MAX(fq.quote_date) as last_quote,
                COUNT(DISTINCT DATE(fq.quote_date, 'start of month')) as active_months
            FROM factories f
            LEFT JOIN factory_quotes fq ON f.id = fq.factory_id
            GROUP BY f.id, f.factory_name, f.location, f.capability
            ORDER BY total_quotes DESC
            """

            factory_df = pd.read_sql_query(factory_query, conn)

            # æŒ‰äº§å“ç±»åˆ«ç»Ÿè®¡å·¥å‚æŠ¥ä»·
            category_query = """
            SELECT
                f.factory_name,
                fq.product_category,
                COUNT(fq.id) as quote_count,
                AVG(fq.price) as avg_price,
                MIN(fq.price) as min_price,
                MAX(fq.price) as max_price
            FROM factories f
            INNER JOIN factory_quotes fq ON f.id = fq.factory_id
            GROUP BY f.id, f.factory_name, fq.product_category
            ORDER BY f.factory_name, quote_count DESC
            """

            category_df = pd.read_sql_query(category_query, conn)

            conn.close()

            result = {
                'factory_summary': factory_df.to_dict('records'),
                'factory_by_category': category_df.to_dict('records'),
                'total_factories': len(factory_df),
                'active_factories': len(factory_df[factory_df['total_quotes'] > 0])
            }

            self.logger.info(f"âœ… å·¥å‚è¡¨ç°ç»Ÿè®¡å®Œæˆ: {len(factory_df)} ä¸ªå·¥å‚")
            return result

        except Exception as e:
            self.logger.error(f"âŒ å·¥å‚è¡¨ç°ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_temporal_analysis(self) -> Dict[str, Any]:
        """æ—¶é—´ç»´åº¦åˆ†æ"""
        self.logger.info("ğŸ“… ç»Ÿè®¡æ—¶é—´ç»´åº¦æ•°æ®...")

        try:
            conn = sqlite3.connect(self.db_path)

            # å®¢æˆ·æ³¨å†Œè¶‹åŠ¿
            customer_trend_query = """
            SELECT
                DATE(created_at) as registration_date,
                COUNT(*) as new_customers,
                COUNT(DISTINCT country) as new_countries
            FROM customers
            WHERE created_at IS NOT NULL
            GROUP BY DATE(created_at)
            ORDER BY registration_date DESC
            LIMIT 90
            """

            customer_trend_df = pd.read_sql_query(customer_trend_query, conn)

            # å›¾çº¸ä¸Šä¼ è¶‹åŠ¿
            drawing_upload_query = """
            SELECT
                DATE(upload_date) as upload_date,
                COUNT(*) as uploaded_drawings,
                COUNT(DISTINCT customer_id) as unique_customers,
                COUNT(DISTINCT product_category) as unique_categories
            FROM drawings
            WHERE upload_date IS NOT NULL
            GROUP BY DATE(upload_date)
            ORDER BY upload_date DESC
            LIMIT 90
            """

            drawing_upload_df = pd.read_sql_query(drawing_upload_query, conn)

            # æŠ¥ä»·æ—¶é—´è¶‹åŠ¿
            quote_trend_query = """
            SELECT
                DATE(quote_date) as quote_date,
                COUNT(*) as quotes_count,
                COUNT(DISTINCT factory_id) as unique_factories,
                AVG(price) as avg_price
            FROM factory_quotes
            WHERE quote_date IS NOT NULL
            GROUP BY DATE(quote_date)
            ORDER BY quote_date DESC
            LIMIT 90
            """

            quote_trend_df = pd.read_sql_query(quote_trend_query, conn)

            conn.close()

            result = {
                'customer_registration_trend': customer_trend_df.to_dict('records'),
                'drawing_upload_trend': drawing_upload_df.to_dict('records'),
                'quote_trend': quote_trend_df.to_dict('records')
            }

            self.logger.info("âœ… æ—¶é—´ç»´åº¦åˆ†æå®Œæˆ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ æ—¶é—´ç»´åº¦åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}

    def get_quality_metrics(self) -> Dict[str, Any]:
        """æ•°æ®è´¨é‡æŒ‡æ ‡"""
        self.logger.info("ğŸ” è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡...")

        try:
            conn = sqlite3.connect(self.db_path)

            # å®¢æˆ·æ•°æ®è´¨é‡
            customer_quality_query = """
            SELECT
                COUNT(*) as total_customers,
                COUNT(CASE WHEN contact_email IS NOT NULL AND contact_email != '' THEN 1 END) as has_email,
                COUNT(CASE WHEN company_name IS NOT NULL AND company_name != '' THEN 1 END) as has_company_name,
                COUNT(CASE WHEN country IS NOT NULL AND country != '' THEN 1 END) as has_country,
                COUNT(CASE WHEN last_inquiry_date IS NOT NULL THEN 1 END) as has_last_inquiry,
                COUNT(CASE WHEN total_drawings > 0 THEN 1 END) as has_drawings
            FROM customers
            """

            customer_quality = pd.read_sql_query(customer_quality_query, conn).iloc[0].to_dict()

            # å›¾çº¸æ•°æ®è´¨é‡
            drawing_quality_query = """
            SELECT
                COUNT(*) as total_drawings,
                COUNT(CASE WHEN drawing_name IS NOT NULL AND drawing_name != '' THEN 1 END) as has_name,
                COUNT(CASE WHEN file_path IS NOT NULL AND file_path != '' THEN 1 END) as has_file_path,
                COUNT(CASE WHEN customer_id IS NOT NULL THEN 1 END) as has_customer,
                COUNT(CASE WHEN product_category IS NOT NULL AND product_category != '' THEN 1 END) as has_category,
                COUNT(CASE WHEN is_classified = 1 THEN 1 END) as is_classified,
                AVG(classification_confidence) as avg_confidence
            FROM drawings
            """

            drawing_quality = pd.read_sql_query(drawing_quality_query, conn).iloc[0].to_dict()

            # å·¥å‚æ•°æ®è´¨é‡
            factory_quality_query = """
            SELECT
                COUNT(*) as total_factories,
                COUNT(CASE WHEN factory_name IS NOT NULL AND factory_name != '' THEN 1 END) as has_name,
                COUNT(CASE WHEN location IS NOT NULL AND location != '' THEN 1 END) as has_location,
                COUNT(CASE WHEN capability IS NOT NULL AND capability != '' THEN 1 END) as has_capability
            FROM factories
            """

            factory_quality = pd.read_sql_query(factory_quality_query, conn).iloc[0].to_dict()

            conn.close()

            # è®¡ç®—è´¨é‡åˆ†æ•°
            customer_quality_score = (
                (customer_quality['has_email'] / customer_quality['total_customers'] * 40) +
                (customer_quality['has_company_name'] / customer_quality['total_customers'] * 30) +
                (customer_quality['has_country'] / customer_quality['total_customers'] * 15) +
                (customer_quality['has_last_inquiry'] / customer_quality['total_customers'] * 15)
            )

            drawing_quality_score = (
                (drawing_quality['has_name'] / drawing_quality['total_drawings'] * 25) +
                (drawing_quality['has_file_path'] / drawing_quality['total_drawings'] * 25) +
                (drawing_quality['has_customer'] / drawing_quality['total_drawings'] * 25) +
                (drawing_quality['has_category'] / drawing_quality['total_drawings'] * 25)
            )

            factory_quality_score = (
                (factory_quality['has_name'] / factory_quality['total_factories'] * 40) +
                (factory_quality['has_location'] / factory_quality['total_factories'] * 30) +
                (factory_quality['has_capability'] / factory_quality['total_factories'] * 30)
            )

            result = {
                'customer_quality': {
                    **customer_quality,
                    'quality_score': round(customer_quality_score, 2),
                    'email_completeness': round(customer_quality['has_email'] / customer_quality['total_customers'] * 100, 2),
                    'name_completeness': round(customer_quality['has_company_name'] / customer_quality['total_customers'] * 100, 2)
                },
                'drawing_quality': {
                    **drawing_quality,
                    'quality_score': round(drawing_quality_score, 2),
                    'classification_rate': round(drawing_quality['is_classified'] / drawing_quality['total_drawings'] * 100, 2),
                    'category_completeness': round(drawing_quality['has_category'] / drawing_quality['total_drawings'] * 100, 2)
                },
                'factory_quality': {
                    **factory_quality,
                    'quality_score': round(factory_quality_score, 2),
                    'name_completeness': round(factory_quality['has_name'] / factory_quality['total_factories'] * 100, 2)
                },
                'overall_quality': round((customer_quality_score + drawing_quality_score + factory_quality_score) / 3, 2)
            }

            self.logger.info("âœ… æ•°æ®è´¨é‡æŒ‡æ ‡è®¡ç®—å®Œæˆ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®è´¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}

    def convert_for_json(self, obj):
        """è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        import numpy as np

        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif hasattr(obj, 'isna') and hasattr(obj, 'any'):
            # å¤„ç†pandaså¸ƒå°”æ•°ç»„å’ŒNAå€¼
            try:
                if obj.any() if hasattr(obj, 'any') else False:
                    return True
                elif obj.all() if hasattr(obj, 'all') else False:
                    return True
                else:
                    return None
            except:
                return None
        elif pd.isna(obj) if not isinstance(obj, (list, dict)) else False:
            return None
        elif isinstance(obj, dict):
            return {k: self.convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_for_json(item) for item in obj]
        else:
            return obj

    def export_to_files(self, statistics: Dict[str, Any]) -> Dict[str, str]:
        """å¯¼å‡ºç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶"""
        self.logger.info("ğŸ’¾ å¯¼å‡ºç»Ÿè®¡ç»“æœ...")

        output_dir = Path("./data/processed")
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exported_files = {}

        try:
            # 1. å¯¼å‡ºå®¢æˆ·çŠ¶æ€ç»Ÿè®¡
            if 'customers_by_status' in statistics:
                customers_file = output_dir / f"customers_by_status_{timestamp}.json"
                with open(customers_file, 'w', encoding='utf-8') as f:
                    json.dump(self.convert_for_json(statistics['customers_by_status']), f, ensure_ascii=False, indent=2)
                exported_files['customers_by_status'] = str(customers_file)

                # åŒæ—¶å¯¼å‡ºCSVæ ¼å¼
                customers_df = pd.DataFrame(statistics['customers_by_status']['detailed'])
                customers_csv_file = output_dir / f"customers_detailed_{timestamp}.csv"
                customers_df.to_csv(customers_csv_file, index=False, encoding='utf-8-sig')
                exported_files['customers_detailed_csv'] = str(customers_csv_file)

            # 2. å¯¼å‡ºå›¾çº¸åˆ†ç±»ç»Ÿè®¡
            if 'drawings_by_category' in statistics:
                drawings_file = output_dir / f"drawings_by_category_{timestamp}.json"
                with open(drawings_file, 'w', encoding='utf-8') as f:
                    json.dump(self.convert_for_json(statistics['drawings_by_category']), f, ensure_ascii=False, indent=2)
                exported_files['drawings_by_category'] = str(drawings_file)

                # å¯¼å‡ºåˆ†ç±»åˆ†å¸ƒCSV
                category_df = pd.DataFrame(statistics['drawings_by_category']['category_distribution'])
                category_csv_file = output_dir / f"drawings_category_distribution_{timestamp}.csv"
                category_df.to_csv(category_csv_file, index=False, encoding='utf-8-sig')
                exported_files['drawings_category_csv'] = str(category_csv_file)

            # 3. å¯¼å‡ºå·¥å‚è¡¨ç°ç»Ÿè®¡
            if 'factory_performance' in statistics:
                factory_file = output_dir / f"factory_performance_{timestamp}.json"
                with open(factory_file, 'w', encoding='utf-8') as f:
                    json.dump(self.convert_for_json(statistics['factory_performance']), f, ensure_ascii=False, indent=2)
                exported_files['factory_performance'] = str(factory_file)

                # å¯¼å‡ºå·¥å‚æ‘˜è¦CSV
                factory_df = pd.DataFrame(statistics['factory_performance']['factory_summary'])
                factory_csv_file = output_dir / f"factory_summary_{timestamp}.csv"
                factory_df.to_csv(factory_csv_file, index=False, encoding='utf-8-sig')
                exported_files['factory_summary_csv'] = str(factory_csv_file)

            # 4. å¯¼å‡ºæ—¶é—´ç»´åº¦åˆ†æ
            if 'temporal_analysis' in statistics:
                temporal_file = output_dir / f"temporal_analysis_{timestamp}.json"
                with open(temporal_file, 'w', encoding='utf-8') as f:
                    json.dump(self.convert_for_json(statistics['temporal_analysis']), f, ensure_ascii=False, indent=2)
                exported_files['temporal_analysis'] = str(temporal_file)

            # 5. å¯¼å‡ºæ•°æ®è´¨é‡æŒ‡æ ‡
            if 'quality_metrics' in statistics:
                quality_file = output_dir / f"quality_metrics_{timestamp}.json"
                with open(quality_file, 'w', encoding='utf-8') as f:
                    json.dump(self.convert_for_json(statistics['quality_metrics']), f, ensure_ascii=False, indent=2)
                exported_files['quality_metrics'] = str(quality_file)

            # 6. å¯¼å‡ºç»¼åˆç»Ÿè®¡æŠ¥å‘Š
            comprehensive_report = {
                'generated_at': datetime.now().isoformat(),
                'statistics_summary': {
                    'customers_by_status': statistics.get('customers_by_status', {}).get('total_customers', 0),
                    'drawings_by_category': statistics.get('drawings_by_category', {}).get('total_drawings', 0),
                    'factory_performance': statistics.get('factory_performance', {}).get('total_factories', 0),
                    'overall_quality_score': statistics.get('quality_metrics', {}).get('overall_quality', 0)
                },
                'detailed_statistics': statistics
            }

            comprehensive_file = output_dir / f"comprehensive_statistics_{timestamp}.json"
            with open(comprehensive_file, 'w', encoding='utf-8') as f:
                json.dump(self.convert_for_json(comprehensive_report), f, ensure_ascii=False, indent=2)
            exported_files['comprehensive_statistics'] = str(comprehensive_file)

            self.logger.info(f"âœ… ç»Ÿè®¡ç»“æœå¯¼å‡ºå®Œæˆ: {len(exported_files)} ä¸ªæ–‡ä»¶")
            return exported_files

        except Exception as e:
            self.logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return {}

    def run_export(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ç»Ÿè®¡å¯¼å‡º"""
        self.logger.info("ğŸš€ å¼€å§‹ç»Ÿè®¡ç»“æœå¯¼å‡º...")

        start_time = datetime.now()

        try:
            # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®
            statistics = {
                'customers_by_status': self.get_customers_by_status(),
                'drawings_by_category': self.get_drawings_by_category(),
                'factory_performance': self.get_factory_performance_stats(),
                'temporal_analysis': self.get_temporal_analysis(),
                'quality_metrics': self.get_quality_metrics()
            }

            # å¯¼å‡ºæ–‡ä»¶
            exported_files = self.export_to_files(statistics)

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            errors = {key: value.get('error') for key, value in statistics.items() if isinstance(value, dict) and 'error' in value}

            result = {
                'success': len(errors) == 0,
                'processing_time': processing_time,
                'statistics_summary': {
                    'customers_count': statistics.get('customers_by_status', {}).get('total_customers', 0),
                    'drawings_count': statistics.get('drawings_by_category', {}).get('total_drawings', 0),
                    'factories_count': statistics.get('factory_performance', {}).get('total_factories', 0),
                    'overall_quality': statistics.get('quality_metrics', {}).get('overall_quality', 0)
                },
                'exported_files': exported_files,
                'errors': errors if errors else None
            }

            if result['success']:
                self.logger.info(f"âœ… ç»Ÿè®¡å¯¼å‡ºå®Œæˆ! è€—æ—¶: {processing_time:.2f}ç§’")
            else:
                self.logger.warning(f"âš ï¸ éƒ¨åˆ†ç»Ÿè®¡å¯¼å‡ºå¤±è´¥ï¼Œé”™è¯¯: {errors}")

            return result

        except Exception as e:
            self.logger.error(f"âŒ ç»Ÿè®¡å¯¼å‡ºå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ç»Ÿè®¡ç»“æœå¯¼å‡ºå·¥å…·')
    parser.add_argument('--db-path', default='./data/db.sqlite', help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='./data/processed', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--format', choices=['json', 'csv', 'all'], default='all', help='å¯¼å‡ºæ ¼å¼')
    parser.add_argument('--stats-type', choices=['customers', 'drawings', 'factory', 'temporal', 'quality', 'all'], default='all', help='ç»Ÿè®¡ç±»å‹')

    args = parser.parse_args()

    exporter = StatisticsExporter(args.db_path)

    result = exporter.run_export()

    if result['success']:
        print("âœ… ç»Ÿè®¡ç»“æœå¯¼å‡ºå®Œæˆ!")
        print(f"ğŸ“Š å®¢æˆ·æ•°: {result['statistics_summary']['customers_count']}")
        print(f"ğŸ“„ å›¾çº¸æ•°: {result['statistics_summary']['drawings_count']}")
        print(f"ğŸ­ å·¥å‚æ•°: {result['statistics_summary']['factories_count']}")
        print(f"ğŸ“ˆ æ•°æ®è´¨é‡: {result['statistics_summary']['overall_quality']}/100")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")

        print("\nğŸ“„ å¯¼å‡ºæ–‡ä»¶:")
        for file_type, file_path in result['exported_files'].items():
            print(f"  {file_type}: {file_path}")
    else:
        print(f"âŒ ç»Ÿè®¡å¯¼å‡ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        if result.get('errors'):
            print("è¯¦ç»†é”™è¯¯:")
            for stat_type, error in result['errors'].items():
                print(f"  {stat_type}: {error}")

if __name__ == "__main__":
    main()