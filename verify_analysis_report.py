#!/usr/bin/env python3
"""
éªŒè¯åˆ†ææŠ¥å‘Šè„šæœ¬
æ‰§è¡Œåˆ†ç±»è„šæœ¬ + åˆ†æè„šæœ¬ â†’ æ£€æŸ¥è¾“å‡ºç»“æœæ˜¯å¦åˆç† â†’ ç¼–å†™éªŒè¯æŠ¥å‘Š
"""

import sqlite3
import json
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import subprocess
import sys

class AnalysisValidator:
    """åˆ†æç»“æœéªŒè¯å™¨"""

    def __init__(self, db_path: str = "./data/db.sqlite"):
        self.db_path = db_path
        self.setup_logging()
        self.validation_results = {}

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("./logs")
        log_dir.mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'verify_analysis.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('AnalysisValidator')

    def run_script_with_validation(self, script_name: str, script_args: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œè„šæœ¬å¹¶éªŒè¯ç»“æœ"""
        self.logger.info(f"ğŸš€ è¿è¡Œè„šæœ¬: {script_name}")

        if script_args is None:
            script_args = []

        try:
            # è¿è¡Œè„šæœ¬
            cmd = [sys.executable, script_name] + script_args
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd="/Users/gavin/Knowledge base"
            )

            execution_result = {
                'script': script_name,
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'execution_time': datetime.now().isoformat()
            }

            if execution_result['success']:
                self.logger.info(f"âœ… {script_name} æ‰§è¡ŒæˆåŠŸ")
            else:
                self.logger.error(f"âŒ {script_name} æ‰§è¡Œå¤±è´¥: {result.stderr}")

            return execution_result

        except Exception as e:
            self.logger.error(f"âŒ è¿è¡Œ {script_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return {
                'script': script_name,
                'success': False,
                'error': str(e),
                'execution_time': datetime.now().isoformat()
            }

    def validate_classification_results(self) -> Dict[str, Any]:
        """éªŒè¯åˆ†ç±»ç»“æœ"""
        self.logger.info("ğŸ” éªŒè¯åˆ†ç±»ç»“æœ...")

        try:
            conn = sqlite3.connect(self.db_path)

            # 1. æ£€æŸ¥åˆ†ç±»è¦†ç›–ç‡
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM drawings")
            total_drawings = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM drawings WHERE is_classified = 1")
            classified_drawings = cursor.fetchone()[0]

            classification_rate = (classified_drawings / total_drawings * 100) if total_drawings > 0 else 0

            # 2. æ£€æŸ¥åˆ†ç±»åˆ†å¸ƒ
            cursor.execute("""
                SELECT product_category, COUNT(*) as count
                FROM drawings
                GROUP BY product_category
                ORDER BY count DESC
            """)
            category_distribution = cursor.fetchall()

            # 3. æ£€æŸ¥æ ‡å‡†ä»¶vså®šåˆ¶ä»¶åˆ†å¸ƒ
            cursor.execute("""
                SELECT
                    standard_or_custom,
                    COUNT(*) as count,
                    COUNT(DISTINCT product_category) as categories
                FROM drawings
                WHERE standard_or_custom IS NOT NULL
                GROUP BY standard_or_custom
            """)
            standard_custom_dist = [(row[0], row[1]) for row in cursor.fetchall()]  # åªå–å‰ä¸¤åˆ—

            # 4. æ£€æŸ¥ç½®ä¿¡åº¦åˆ†å¸ƒ
            cursor.execute("""
                SELECT
                    CASE
                        WHEN classification_confidence >= 0.8 THEN 'é«˜'
                        WHEN classification_confidence >= 0.5 THEN 'ä¸­'
                        WHEN classification_confidence > 0 THEN 'ä½'
                        ELSE 'æ— '
                    END as confidence_level,
                    COUNT(*) as count
                FROM drawings
                GROUP BY confidence_level
            """)
            confidence_distribution = cursor.fetchall()

            # 5. æ£€æŸ¥æ•°æ®æºåˆ†å¸ƒ
            cursor.execute("""
                SELECT data_source, COUNT(*) as count
                FROM drawings
                WHERE data_source IS NOT NULL AND data_source != 'unknown'
                GROUP BY data_source
                ORDER BY count DESC
            """)
            source_distribution = cursor.fetchall()

            conn.close()

            # éªŒè¯è§„åˆ™
            validation_rules = {
                'coverage_rate_acceptable': classification_rate >= 5,  # è‡³å°‘5%åˆ†ç±»ç‡
                'has_multiple_categories': len(category_distribution) >= 2,
                'has_custom_items': any(row[0] == 1 for row in standard_custom_dist),  # æœ‰å®šåˆ¶ä»¶
                'has_confidence_scores': any(row[0] != 'æ— ' for row in confidence_distribution),
                'reasonable_distribution': self._validate_category_distribution(category_distribution)
            }

            validation_result = {
                'classification_metrics': {
                    'total_drawings': total_drawings,
                    'classified_drawings': classified_drawings,
                    'classification_rate': round(classification_rate, 2),
                    'category_distribution': {cat: count for cat, count in category_distribution},
                    'standard_custom_distribution': {std: count for std, count in standard_custom_dist},
                    'confidence_distribution': {conf: count for conf, count in confidence_distribution},
                    'source_distribution': {src: count for src, count in source_distribution}
                },
                'validation_rules': validation_rules,
                'overall_status': 'PASS' if all(validation_rules.values()) else 'NEEDS_ATTENTION'
            }

            self.logger.info(f"âœ… åˆ†ç±»éªŒè¯å®Œæˆ: {validation_result['overall_status']}")
            return validation_result

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ç±»éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'overall_status': 'ERROR'}

    def _validate_category_distribution(self, distribution: List[Tuple]) -> bool:
        """éªŒè¯åˆ†ç±»åˆ†å¸ƒæ˜¯å¦åˆç†"""
        if not distribution:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»å¯¼åˆ†ç±»
        total_count = sum(count for _, count in distribution)
        if total_count == 0:
            return False

        # ä¸»å¯¼åˆ†ç±»ä¸åº”è¶…è¿‡90%
        max_count = max(count for _, count in distribution)
        if max_count / total_count > 0.9:
            return False

        # è‡³å°‘æœ‰ä¸€ä¸ªåˆç†çš„åˆ†ç±»ï¼ˆä¸æ˜¯æœªåˆ†ç±»ï¼‰
        reasonable_categories = sum(1 for category, count in distribution
                                  if category != 'æœªåˆ†ç±»' and count > 0)

        return reasonable_categories >= 1

    def validate_analysis_results(self) -> Dict[str, Any]:
        """éªŒè¯åˆ†æç»“æœ"""
        self.logger.info("ğŸ” éªŒè¯åˆ†æç»“æœ...")

        try:
            # æ£€æŸ¥åˆ†æè¾“å‡ºæ–‡ä»¶
            processed_dir = Path("./data/processed")
            analysis_files = list(processed_dir.glob("*analysis*"))
            trend_files = list(processed_dir.glob("*trends*"))
            statistics_files = list(processed_dir.glob("*statistics*"))

            validation_result = {
                'output_files': {
                    'analysis_files': len(analysis_files),
                    'trend_files': len(trend_files),
                    'statistics_files': len(statistics_files),
                    'total_files': len(analysis_files) + len(trend_files) + len(statistics_files)
                },
                'file_validation': self._validate_output_files(analysis_files + trend_files + statistics_files)
            }

            # éªŒè¯æ•°æ®åº“ä¸­çš„åˆ†ææ•°æ®
            db_validation = self._validate_database_analysis_data()
            validation_result.update(db_validation)

            self.logger.info(f"âœ… åˆ†æéªŒè¯å®Œæˆ: {validation_result.get('overall_status', 'UNKNOWN')}")
            return validation_result

        except Exception as e:
            self.logger.error(f"âŒ åˆ†æéªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'overall_status': 'ERROR'}

    def _validate_output_files(self, file_list: List[Path]) -> Dict[str, Any]:
        """éªŒè¯è¾“å‡ºæ–‡ä»¶"""
        file_validation = {
            'total_files': len(file_list),
            'valid_files': 0,
            'invalid_files': 0,
            'file_details': []
        }

        for file_path in file_list:
            try:
                if file_path.suffix == '.json':
                    # éªŒè¯JSONæ–‡ä»¶
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    file_validation['valid_files'] += 1
                    file_validation['file_details'].append({
                        'file': file_path.name,
                        'type': 'json',
                        'size': file_path.stat().st_size,
                        'valid': True
                    })

                elif file_path.suffix == '.csv':
                    # éªŒè¯CSVæ–‡ä»¶
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                    if len(df) > 0:
                        file_validation['valid_files'] += 1
                        file_validation['file_details'].append({
                            'file': file_path.name,
                            'type': 'csv',
                            'rows': len(df),
                            'columns': len(df.columns),
                            'valid': True
                        })
                    else:
                        file_validation['invalid_files'] += 1
                        file_validation['file_details'].append({
                            'file': file_path.name,
                            'type': 'csv',
                            'valid': False,
                            'issue': 'Empty file'
                        })
                else:
                    file_validation['invalid_files'] += 1

            except Exception as e:
                file_validation['invalid_files'] += 1
                file_validation['file_details'].append({
                    'file': file_path.name,
                    'valid': False,
                    'issue': str(e)
                })

        return file_validation

    def _validate_database_analysis_data(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®åº“ä¸­çš„åˆ†ææ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            validation_data = {}

            # éªŒè¯å·¥å‚æŠ¥ä»·æ—¶é—´å­—æ®µ
            cursor.execute("""
                SELECT COUNT(*) FROM factory_quotes
                WHERE quote_month IS NOT NULL AND quote_year IS NOT NULL
            """)
            quotes_with_time_fields = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM factory_quotes")
            total_quotes = cursor.fetchone()[0]

            validation_data['quote_time_fields'] = {
                'total_quotes': total_quotes,
                'quotes_with_time_fields': quotes_with_time_fields,
                'completion_rate': (quotes_with_time_fields / total_quotes * 100) if total_quotes > 0 else 0
            }

            # éªŒè¯å›¾çº¸åˆ†ç±»å­—æ®µ
            cursor.execute("""
                SELECT COUNT(*) FROM drawings
                WHERE is_classified = 1 AND classification_date IS NOT NULL
            """)
            drawings_with_classification = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM drawings")
            total_drawings = cursor.fetchone()[0]

            validation_data['drawing_classification'] = {
                'total_drawings': total_drawings,
                'drawings_with_classification': drawings_with_classification,
                'classification_rate': (drawings_with_classification / total_drawings * 100) if total_drawings > 0 else 0
            }

            # éªŒè¯å®¢æˆ·ç»Ÿè®¡å­—æ®µ
            cursor.execute("""
                SELECT COUNT(*) FROM customers
                WHERE total_drawings >= 0
            """)
            customers_with_stats = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM customers")
            total_customers = cursor.fetchone()[0]

            validation_data['customer_statistics'] = {
                'total_customers': total_customers,
                'customers_with_stats': customers_with_stats,
                'stats_completion_rate': (customers_with_stats / total_customers * 100) if total_customers > 0 else 0
            }

            conn.close()

            # æ•´ä½“çŠ¶æ€
            overall_scores = [
                validation_data['quote_time_fields']['completion_rate'],
                validation_data['drawing_classification']['classification_rate'],
                validation_data['customer_statistics']['stats_completion_rate']
            ]

            validation_data['overall_analysis_quality'] = sum(overall_scores) / len(overall_scores)
            validation_data['overall_status'] = 'GOOD' if validation_data['overall_analysis_quality'] >= 80 else 'NEEDS_IMPROVEMENT'

            return validation_data

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“åˆ†ææ•°æ®éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'overall_status': 'ERROR'}

    def validate_data_quality(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è´¨é‡"""
        self.logger.info("ğŸ” éªŒè¯æ•°æ®è´¨é‡...")

        try:
            # è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬
            quality_result = self.run_script_with_validation('data_quality_check.py')

            if quality_result['success']:
                # è§£æè´¨é‡æ£€æŸ¥ç»“æœ
                quality_output = quality_result['stdout']

                quality_validation = {
                    'script_execution': 'SUCCESS',
                    'quality_issues_detected': 'âŒ' in quality_output,
                    'data_quality_grade': self._extract_quality_grade(quality_output),
                    'recommendations': self._extract_recommendations(quality_output)
                }
            else:
                quality_validation = {
                    'script_execution': 'FAILED',
                    'error': quality_result.get('stderr', 'Unknown error')
                }

            self.logger.info(f"âœ… æ•°æ®è´¨é‡éªŒè¯å®Œæˆ: {quality_validation.get('script_execution', 'UNKNOWN')}")
            return quality_validation

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'script_execution': 'ERROR'}

    def _extract_quality_grade(self, output: str) -> str:
        """ä»è¾“å‡ºä¸­æå–æ•°æ®è´¨é‡ç­‰çº§"""
        if 'æ•°æ®è´¨é‡ç­‰çº§: A' in output:
            return 'A'
        elif 'æ•°æ®è´¨é‡ç­‰çº§: B' in output:
            return 'B'
        elif 'æ•°æ®è´¨é‡ç­‰çº§: C' in output:
            return 'C'
        elif 'æ•°æ®è´¨é‡ç­‰çº§: D' in output:
            return 'D'
        else:
            return 'UNKNOWN'

    def _extract_recommendations(self, output: str) -> List[str]:
        """ä»è¾“å‡ºä¸­æå–å»ºè®®"""
        recommendations = []
        lines = output.split('\n')

        for line in lines:
            if line.strip().startswith('- '):
                recommendations.append(line.strip())

        return recommendations

    def generate_validation_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        self.logger.info("ğŸ“„ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")

        try:
            report_dir = Path("./data/processed")
            report_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"validation_report_{timestamp}.json"

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = {
                'validation_timestamp': datetime.now().isoformat(),
                'execution_summary': {
                    'total_scripts_executed': len([r for r in results.get('script_executions', []) if r.get('success')]),
                    'total_scripts_failed': len([r for r in results.get('script_executions', []) if not r.get('success')]),
                    'overall_success_rate': len([r for r in results.get('script_executions', []) if r.get('success')]) / len(results.get('script_executions', [])) * 100 if results.get('script_executions') else 0
                },
                'classification_validation': results.get('classification_validation', {}),
                'analysis_validation': results.get('analysis_validation', {}),
                'data_quality_validation': results.get('data_quality_validation', {}),
                'overall_assessment': self._generate_overall_assessment(results),
                'recommendations': self._generate_recommendations(results),
                'next_steps': self._generate_next_steps(results)
            }

            # ä¿å­˜JSONæŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)

            # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
            text_report = self._generate_text_report(comprehensive_report)
            text_report_file = report_dir / f"validation_report_summary_{timestamp}.txt"

            with open(text_report_file, 'w', encoding='utf-8') as f:
                f.write(text_report)

            self.logger.info(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return str(report_file)

        except Exception as e:
            self.logger.error(f"âŒ ç”ŸæˆéªŒè¯æŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def _generate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        assessment = {
            'overall_status': 'UNKNOWN',
            'classification_quality': 'UNKNOWN',
            'analysis_quality': 'UNKNOWN',
            'data_quality': 'UNKNOWN',
            'system_health': 'UNKNOWN'
        }

        # åˆ†ç±»è´¨é‡è¯„ä¼°
        classification_val = results.get('classification_validation', {})
        if classification_val.get('overall_status') == 'PASS':
            assessment['classification_quality'] = 'GOOD'
        elif classification_val.get('overall_status') == 'NEEDS_ATTENTION':
            assessment['classification_quality'] = 'FAIR'
        else:
            assessment['classification_quality'] = 'POOR'

        # åˆ†æè´¨é‡è¯„ä¼°
        analysis_val = results.get('analysis_validation', {})
        if analysis_val.get('overall_status') == 'GOOD':
            assessment['analysis_quality'] = 'GOOD'
        elif analysis_val.get('overall_status') == 'NEEDS_IMPROVEMENT':
            assessment['analysis_quality'] = 'FAIR'
        else:
            assessment['analysis_quality'] = 'POOR'

        # æ•°æ®è´¨é‡è¯„ä¼°
        data_quality_val = results.get('data_quality_validation', {})
        if data_quality_val.get('script_execution') == 'SUCCESS':
            grade = data_quality_val.get('data_quality_grade', 'UNKNOWN')
            if grade in ['A', 'B']:
                assessment['data_quality'] = 'GOOD'
            elif grade == 'C':
                assessment['data_quality'] = 'FAIR'
            else:
                assessment['data_quality'] = 'POOR'
        else:
            assessment['data_quality'] = 'POOR'

        # ç³»ç»Ÿå¥åº·è¯„ä¼°
        scores = []
        if assessment['classification_quality'] == 'GOOD':
            scores.append(1)
        elif assessment['classification_quality'] == 'FAIR':
            scores.append(0.5)
        else:
            scores.append(0)

        if assessment['analysis_quality'] == 'GOOD':
            scores.append(1)
        elif assessment['analysis_quality'] == 'FAIR':
            scores.append(0.5)
        else:
            scores.append(0)

        if assessment['data_quality'] == 'GOOD':
            scores.append(1)
        elif assessment['data_quality'] == 'FAIR':
            scores.append(0.5)
        else:
            scores.append(0)

        overall_score = sum(scores) / len(scores) if scores else 0

        if overall_score >= 0.8:
            assessment['overall_status'] = 'EXCELLENT'
            assessment['system_health'] = 'HEALTHY'
        elif overall_score >= 0.6:
            assessment['overall_status'] = 'GOOD'
            assessment['system_health'] = 'HEALTHY'
        elif overall_score >= 0.4:
            assessment['overall_status'] = 'FAIR'
            assessment['system_health'] = 'NEEDS_ATTENTION'
        else:
            assessment['overall_status'] = 'POOR'
            assessment['system_health'] = 'CRITICAL'

        assessment['overall_score'] = round(overall_score * 100, 2)

        return assessment

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åˆ†ç±»æ”¹è¿›å»ºè®®
        classification_val = results.get('classification_validation', {})
        if classification_val.get('overall_status') != 'PASS':
            recommendations.append("æ”¹è¿›åˆ†ç±»è§„åˆ™ï¼Œæé«˜åˆ†ç±»è¦†ç›–ç‡")
            recommendations.append("æ‰©å……å…³é”®è¯åº“ï¼Œå¢å¼ºåˆ†ç±»å‡†ç¡®æ€§")

        # åˆ†ææ”¹è¿›å»ºè®®
        analysis_val = results.get('analysis_validation', {})
        if analysis_val.get('overall_status') != 'GOOD':
            recommendations.append("å®Œå–„åˆ†æè„šæœ¬ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§")
            recommendations.append("å¢åŠ æ›´å¤šç»´åº¦çš„åˆ†ææŒ‡æ ‡")

        # æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®
        data_quality_val = results.get('data_quality_validation', {})
        if data_quality_val.get('script_execution') != 'SUCCESS':
            recommendations.append("ä¿®å¤æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬")
        else:
            recommendations.extend(data_quality_val.get('recommendations', []))

        # é€šç”¨å»ºè®®
        recommendations.append("å®šæœŸæ‰§è¡ŒéªŒè¯æµç¨‹ï¼Œç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€")
        recommendations.append("å»ºç«‹è‡ªåŠ¨åŒ–ç›‘æ§å’Œé¢„è­¦æœºåˆ¶")

        return recommendations

    def _generate_next_steps(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’"""
        next_steps = []

        assessment = self._generate_overall_assessment(results)

        if assessment['overall_status'] in ['EXCELLENT', 'GOOD']:
            next_steps.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œç»§ç»­æŒ‰è®¡åˆ’æ‰§è¡Œå®šæœŸç»´æŠ¤")
            next_steps.append("å‡†å¤‡è¿›å…¥ä¸‹ä¸€é˜¶æ®µå¼€å‘ï¼ˆAPIæ¥å£è®¾è®¡ï¼‰")
        elif assessment['overall_status'] == 'FAIR':
            next_steps.append("ä¼˜å…ˆè§£å†³æ•°æ®è´¨é‡é—®é¢˜")
            next_steps.append("ä¼˜åŒ–åˆ†ç±»ç®—æ³•ï¼Œæé«˜å‡†ç¡®ç‡")
            next_steps.append("å®Œå–„åˆ†æè„šæœ¬åŠŸèƒ½")
        else:
            next_steps.append("ç«‹å³ä¿®å¤å…³é”®é—®é¢˜")
            next_steps.append("é‡æ–°è®¾è®¡æ•°æ®æµç¨‹")
            next_steps.append("åŠ å¼ºæµ‹è¯•å’ŒéªŒè¯æœºåˆ¶")

        return next_steps

    def _generate_text_report(self, report: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        text = f"""
çŸ¥è¯†åº“ç³»ç»ŸéªŒè¯æŠ¥å‘Š
{'=' * 50}
ç”Ÿæˆæ—¶é—´: {report['validation_timestamp']}

æ€»ä½“è¯„ä¼°
--------
ç³»ç»ŸçŠ¶æ€: {report['overall_assessment']['overall_status']}
å¥åº·è¯„åˆ†: {report['overall_assessment']['overall_score']}/100
åˆ†ç±»è´¨é‡: {report['overall_assessment']['classification_quality']}
åˆ†æè´¨é‡: {report['overall_assessment']['analysis_quality']}
æ•°æ®è´¨é‡: {report['overall_assessment']['data_quality']}

æ‰§è¡Œæ‘˜è¦
--------
è„šæœ¬æ‰§è¡ŒæˆåŠŸç‡: {report['execution_summary']['overall_success_rate']:.1f}%
æˆåŠŸæ‰§è¡Œ: {report['execution_summary']['total_scripts_executed']} ä¸ª
æ‰§è¡Œå¤±è´¥: {report['execution_summary']['total_scripts_failed']} ä¸ª

åˆ†ç±»éªŒè¯ç»“æœ
--------
åˆ†ç±»çŠ¶æ€: {report['classification_validation'].get('overall_status', 'UNKNOWN')}
åˆ†ç±»è¦†ç›–ç‡: {report['classification_validation'].get('classification_metrics', {}).get('classification_rate', 0):.1f}%

åˆ†æéªŒè¯ç»“æœ
--------
åˆ†æçŠ¶æ€: {report['analysis_validation'].get('overall_status', 'UNKNOWN')}
è¾“å‡ºæ–‡ä»¶æ•°: {report['analysis_validation'].get('output_files', {}).get('total_files', 0)}

æ•°æ®è´¨é‡éªŒè¯
--------
éªŒè¯çŠ¶æ€: {report['data_quality_validation'].get('script_execution', 'UNKNOWN')}
è´¨é‡ç­‰çº§: {report['data_quality_validation'].get('data_quality_grade', 'UNKNOWN')}

æ”¹è¿›å»ºè®®
--------
{chr(10).join(f"- {rec}" for rec in report.get('recommendations', []))}

ä¸‹ä¸€æ­¥è¡ŒåŠ¨
--------
{chr(10).join(f"- {step}" for step in report.get('next_steps', []))}

{'=' * 50}
æŠ¥å‘Šç”Ÿæˆå®Œæˆ
"""
        return text

    def run_full_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹"""
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´éªŒè¯æµç¨‹...")

        start_time = datetime.now()

        try:
            # 1. æ‰§è¡Œåˆ†ç±»è„šæœ¬
            self.logger.info("ğŸ“ æ‰§è¡Œåˆ†ç±»è„šæœ¬...")
            classification_result = self.run_script_with_validation('classify_drawings.py', ['--report'])

            # 2. æ‰§è¡Œåˆ†æè„šæœ¬
            self.logger.info("ğŸ“Š æ‰§è¡Œåˆ†æè„šæœ¬...")
            analysis_result = self.run_script_with_validation('analyze_factory_quote_trends.py')

            # 3. æ‰§è¡Œç»Ÿè®¡å¯¼å‡ºè„šæœ¬
            self.logger.info("ğŸ’¾ æ‰§è¡Œç»Ÿè®¡å¯¼å‡ºè„šæœ¬...")
            statistics_result = self.run_script_with_validation('export_statistics.py')

            # 4. éªŒè¯ç»“æœ
            self.logger.info("ğŸ” éªŒè¯æ‰§è¡Œç»“æœ...")
            classification_validation = self.validate_classification_results()
            analysis_validation = self.validate_analysis_results()
            data_quality_validation = self.validate_data_quality()

            # 5. ç”ŸæˆæŠ¥å‘Š
            self.logger.info("ğŸ“„ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
            validation_results = {
                'script_executions': {
                    'classification': classification_result,
                    'analysis': analysis_result,
                    'statistics': statistics_result
                },
                'classification_validation': classification_validation,
                'analysis_validation': analysis_validation,
                'data_quality_validation': data_quality_validation
            }

            report_file = self.generate_validation_report(validation_results)

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            final_result = {
                'success': True,
                'processing_time': processing_time,
                'validation_report': report_file,
                'overall_assessment': self._generate_overall_assessment(validation_results),
                'validation_results': validation_results
            }

            self.logger.info(f"âœ… å®Œæ•´éªŒè¯æµç¨‹å®Œæˆ! è€—æ—¶: {processing_time:.2f}ç§’")
            self.logger.info(f"ğŸ“„ éªŒè¯æŠ¥å‘Š: {report_file}")

            return final_result

        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯æµç¨‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': (datetime.now() - start_time).total_seconds()
            }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='åˆ†æç»“æœéªŒè¯å·¥å…·')
    parser.add_argument('--db-path', default='./data/db.sqlite', help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--component', choices=['classification', 'analysis', 'quality', 'all'], default='all', help='éªŒè¯ç»„ä»¶')
    parser.add_argument('--skip-execution', action='store_true', help='è·³è¿‡è„šæœ¬æ‰§è¡Œï¼Œä»…éªŒè¯ç°æœ‰ç»“æœ')

    args = parser.parse_args()

    validator = AnalysisValidator(args.db_path)

    if args.skip_execution:
        # ä»…éªŒè¯ç°æœ‰ç»“æœ
        classification_validation = validator.validate_classification_results()
        analysis_validation = validator.validate_analysis_results()
        data_quality_validation = validator.validate_data_quality()

        results = {
            'classification_validation': classification_validation,
            'analysis_validation': analysis_validation,
            'data_quality_validation': data_quality_validation
        }

        report_file = validator.generate_validation_report(results)
        print(f"âœ… éªŒè¯å®Œæˆ! æŠ¥å‘Š: {report_file}")

    else:
        # è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹
        result = validator.run_full_validation()

        if result['success']:
            print("âœ… åˆ†æéªŒè¯å®Œæˆ!")
            print(f"ğŸ“Š æ€»ä½“è¯„ä¼°: {result['overall_assessment']['overall_status']}")
            print(f"ğŸ’¯ å¥åº·è¯„åˆ†: {result['overall_assessment']['overall_score']}/100")
            print(f"â±ï¸ å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
            print(f"ğŸ“„ éªŒè¯æŠ¥å‘Š: {result['validation_report']}")
        else:
            print(f"âŒ éªŒè¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

if __name__ == "__main__":
    main()