#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
API Chat Interface - èŠå¤©æ¥å£é›†æˆ
é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®¢æˆ·ã€äº§å“ã€æŠ¥ä»·å†å²ç­‰çŸ¥è¯†åº“ä¿¡æ¯

This script provides a Flask API extension for natural language querying
of the knowledge base using semantic search and intelligent response generation.
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import sqlite3
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path
import numpy as np

# Import our knowledge modules
from build_embeddings import EmbeddingIndexBuilder

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/processed/chat_interface.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ChatInterface:
    """èŠå¤©æ¥å£å¤„ç†å™¨"""

    def __init__(self, db_path: str = "knowledge_base.db"):
        self.db_path = db_path
        self.conn = None
        self.embedding_builder = None

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA foreign_keys = ON")
            logger.info(f"Connected to database: {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")

    def init_embedding_search(self):
        """åˆå§‹åŒ–åµŒå…¥æœç´¢"""
        try:
            self.embedding_builder = EmbeddingIndexBuilder()
            self.embedding_builder.connect()
            logger.info("Embedding search initialized")
        except Exception as e:
            logger.error(f"Failed to initialize embedding search: {e}")

    def parse_user_query(self, query: str) -> Dict[str, Any]:
        """è§£æç”¨æˆ·æŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        parsed_query = {
            'original_query': query,
            'intent': 'general_search',
            'entities': [],
            'filters': {},
            'keywords': []
        }

        # è¯†åˆ«æŸ¥è¯¢æ„å›¾
        if any(keyword in query_lower for keyword in ['å®¢æˆ·', 'customer', 'å…¬å¸']):
            parsed_query['intent'] = 'customer_search'
            parsed_query['filters']['entity_types'] = ['customer']

        elif any(keyword in query_lower for keyword in ['æŠ¥ä»·', 'quote', 'ä»·æ ¼', 'price']):
            parsed_query['intent'] = 'quote_search'
            parsed_query['filters']['entity_types'] = ['quote']

        elif any(keyword in query_lower for keyword in ['äº§å“', 'product', 'è§„æ ¼', 'specification']):
            parsed_query['intent'] = 'product_search'
            parsed_query['filters']['entity_types'] = ['product', 'specification']

        elif any(keyword in query_lower for keyword in ['å·¥å‚', 'factory', 'ä¾›åº”å•†', 'supplier']):
            parsed_query['intent'] = 'factory_search'
            parsed_query['filters']['entity_types'] = ['factory']

        elif any(keyword in query_lower for keyword in ['è¯¢ä»·', 'inquiry', 'éœ€æ±‚']):
            parsed_query['intent'] = 'inquiry_search'
            parsed_query['filters']['entity_types'] = ['inquiry']

        # æå–å…³é”®è¯
        keywords = self._extract_keywords(query)
        parsed_query['keywords'] = keywords

        # æå–å®ä½“ï¼ˆå¦‚äº§å“è§„æ ¼ã€ææ–™ç­‰ï¼‰
        entities = self._extract_entities(query)
        parsed_query['entities'] = entities

        return parsed_query

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        # ç§»é™¤åœç”¨è¯å¹¶æå–é‡è¦å…³é”®è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}

        words = re.findall(r'[\w\u4e00-\u9fff]+', query.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 1]

        return keywords[:10]  # é™åˆ¶å…³é”®è¯æ•°é‡

    def _extract_entities(self, query: str) -> List[Dict[str, str]]:
        """æå–å®ä½“ä¿¡æ¯"""
        entities = []

        # æå–è§„æ ¼ä¿¡æ¯
        spec_patterns = [
            (r'm\d+\.?\d*', 'specification'),
            (r'Ï†\d+\.?\d*', 'specification'),
            (r'\d+\.?\d*mm', 'specification'),
            (r'gb/t\s*\d+', 'standard'),
            (r'iso\s*\d+', 'standard'),
        ]

        for pattern, entity_type in spec_patterns:
            matches = re.findall(pattern, query.lower())
            for match in matches:
                entities.append({
                    'type': entity_type,
                    'value': match,
                    'confidence': 0.9
                })

        # æå–ææ–™ä¿¡æ¯
        materials = ['ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“œ', 'é“', '304', '316', '45#']
        for material in materials:
            if material in query.lower():
                entities.append({
                    'type': 'material',
                    'value': material,
                    'confidence': 0.8
                })

        return entities

    def search_knowledge_base(self, parsed_query: Dict) -> List[Dict]:
        """æœç´¢çŸ¥è¯†åº“"""
        try:
            # ä½¿ç”¨åµŒå…¥æœç´¢
            if self.embedding_builder:
                semantic_results = self.embedding_builder.find_similar_entries(
                    parsed_query['original_query'],
                    top_k=10,
                    min_similarity=0.01
                )

                # åº”ç”¨è¿‡æ»¤å™¨
                if parsed_query['filters'].get('entity_types'):
                    semantic_results = [
                        result for result in semantic_results
                        if result['entity_type'] in parsed_query['filters']['entity_types']
                    ]

                # è·å–è¯¦ç»†ä¿¡æ¯
                detailed_results = []
                for result in semantic_results:
                    entry_details = self._get_knowledge_entry_details(result['entry_id'])
                    if entry_details:
                        entry_details['similarity_score'] = result['similarity']
                        detailed_results.append(entry_details)

                return detailed_results

            # å¦‚æœæ²¡æœ‰åµŒå…¥æœç´¢ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢
            return self._traditional_search(parsed_query)

        except Exception as e:
            logger.error(f"Failed to search knowledge base: {e}")
            return []

    def _traditional_search(self, parsed_query: Dict) -> List[Dict]:
        """ä¼ ç»Ÿæ–‡æœ¬æœç´¢"""
        try:
            cursor = self.conn.cursor()

            # æ„å»ºæœç´¢æŸ¥è¯¢
            query_conditions = []
            params = []

            # å®ä½“ç±»å‹è¿‡æ»¤
            if parsed_query['filters'].get('entity_types'):
                placeholders = ','.join(['?'] * len(parsed_query['filters']['entity_types']))
                query_conditions.append(f"ke.entity_type IN ({placeholders})")
                params.extend(parsed_query['filters']['entity_types'])

            # å…³é”®è¯æœç´¢
            if parsed_query['keywords']:
                keyword_conditions = []
                for keyword in parsed_query['keywords']:
                    keyword_conditions.append("(ke.name LIKE ? OR ke.description LIKE ?)")
                    params.extend([f"%{keyword}%", f"%{keyword}%"])
                if keyword_conditions:
                    query_conditions.append(f"({' OR '.join(keyword_conditions)})")

            # æ„å»ºå®Œæ•´æŸ¥è¯¢
            if query_conditions:
                where_clause = " AND ".join(query_conditions)
                query = f"""
                    SELECT ke.id, ke.entity_type, ke.name, ke.description, ke.attributes_json,
                           ke.created_at, et.display_name, et.color, et.icon
                    FROM knowledge_entries ke
                    JOIN entity_types et ON ke.entity_type = et.name
                    WHERE {where_clause}
                    ORDER BY ke.created_at DESC
                    LIMIT 20
                """
            else:
                query = """
                    SELECT ke.id, ke.entity_type, ke.name, ke.description, ke.attributes_json,
                           ke.created_at, et.display_name, et.color, et.icon
                    FROM knowledge_entries ke
                    JOIN entity_types et ON ke.entity_type = et.name
                    ORDER BY ke.created_at DESC
                    LIMIT 10
                """

            cursor.execute(query, params)
            results = []

            for row in cursor.fetchall():
                entry = {
                    'id': row[0],
                    'entity_type': row[1],
                    'name': row[2],
                    'description': row[3],
                    'attributes': json.loads(row[4]) if row[4] else {},
                    'created_at': row[5],
                    'entity_type_display': row[6],
                    'entity_color': row[7],
                    'entity_icon': row[8],
                    'similarity_score': 0.5  # ä¼ ç»Ÿæœç´¢å›ºå®šåˆ†æ•°
                }
                results.append(entry)

            return results

        except Exception as e:
            logger.error(f"Failed to perform traditional search: {e}")
            return []

    def _get_knowledge_entry_details(self, entry_id: int) -> Optional[Dict]:
        """è·å–çŸ¥è¯†æ¡ç›®è¯¦ç»†ä¿¡æ¯"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT ke.id, ke.entity_type, ke.name, ke.description, ke.attributes_json,
                       ke.created_at, et.display_name, et.color, et.icon
                FROM knowledge_entries ke
                JOIN entity_types et ON ke.entity_type = et.name
                WHERE ke.id = ?
            """, (entry_id,))

            row = cursor.fetchone()
            if not row:
                return None

            entry = {
                'id': row[0],
                'entity_type': row[1],
                'name': row[2],
                'description': row[3],
                'attributes': json.loads(row[4]) if row[4] else {},
                'created_at': row[5],
                'entity_type_display': row[6],
                'entity_color': row[7],
                'entity_icon': row[8]
            }

            # è·å–ç›¸å…³çš„NLPå®ä½“
            cursor.execute("""
                SELECT keyword, value, category, confidence_score
                FROM nlp_entities
                WHERE entry_id = ?
                ORDER BY confidence_score DESC
                LIMIT 5
            """, (entry_id,))
            entry['nlp_entities'] = [dict(zip(['keyword', 'value', 'category', 'confidence'], row))
                                 for row in cursor.fetchall()]

            # è·å–ç›¸å…³çš„ç­–ç•¥å»ºè®®
            cursor.execute("""
                SELECT title, description, confidence_score, potential_savings
                FROM strategy_suggestions
                WHERE related_entry_id = ?
                ORDER BY created_at DESC
                LIMIT 3
            """, (entry_id,))
            entry['strategy_suggestions'] = [dict(zip(['title', 'description', 'confidence_score', 'potential_savings'], row))
                                            for row in cursor.fetchall()]

            return entry

        except Exception as e:
            logger.error(f"Failed to get knowledge entry details: {e}")
            return None

    def generate_response(self, parsed_query: Dict, search_results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå“åº”"""
        try:
            if not search_results:
                return {
                    'status': 'no_results',
                    'message': f"å¾ˆæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{parsed_query['original_query']}'çš„ç›¸å…³ä¿¡æ¯ã€‚",
                    'suggestions': [
                        "å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯",
                        "æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®",
                        "æä¾›æ›´å…·ä½“çš„æè¿°"
                    ],
                    'results': []
                }

            # æ ¹æ®æŸ¥è¯¢æ„å›¾ç”Ÿæˆç»“æ„åŒ–å“åº”
            response = {
                'status': 'success',
                'query_intent': parsed_query['intent'],
                'query_entities': parsed_query['entities'],
                'total_results': len(search_results),
                'results': search_results,
                'summary': self._generate_summary(parsed_query, search_results),
                'related_topics': self._find_related_topics(search_results)
            }

            return response

        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            return {
                'status': 'error',
                'message': "ç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                'results': []
            }

    def _generate_summary(self, parsed_query: Dict, search_results: List[Dict]) -> str:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        try:
            if not search_results:
                return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

            # æŒ‰å®ä½“ç±»å‹åˆ†ç»„
            type_counts = {}
            for result in search_results:
                entity_type = result['entity_type_display']
                type_counts[entity_type] = type_counts.get(entity_type, 0) + 1

            # æ„å»ºæ‘˜è¦
            summary_parts = []
            summary_parts.append(f"æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯")

            for entity_type, count in type_counts.items():
                summary_parts.append(f"{count} ä¸ª{entity_type}")

            if len(search_results) > 0:
                top_result = search_results[0]
                if top_result['similarity_score'] > 0.7:
                    summary_parts.append(f"æœ€ç›¸å…³çš„ç»“æœï¼š{top_result['name']}")

            return "ï¼Œ".join(summary_parts) + "ã€‚"

        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            return f"æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯ã€‚"

    def _find_related_topics(self, search_results: List[Dict]) -> List[str]:
        """æŸ¥æ‰¾ç›¸å…³è¯é¢˜"""
        try:
            topics = set()

            for result in search_results:
                # ä»å®ä½“ç±»å‹ç”Ÿæˆè¯é¢˜
                topics.add(result['entity_type_display'])

                # ä»å±æ€§ç”Ÿæˆè¯é¢˜
                if result.get('attributes'):
                    for key, value in result['attributes'].items():
                        if key in ['materials', 'specifications', 'industry']:
                            if isinstance(value, list):
                                topics.update(value)
                            elif value:
                                topics.add(str(value))

            return list(topics)[:8]  # é™åˆ¶è¯é¢˜æ•°é‡

        except Exception as e:
            logger.error(f"Failed to find related topics: {e}")
            return []

    def process_query(self, query: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            logger.info(f"Processing query: {query}")

            # è§£ææŸ¥è¯¢
            parsed_query = self.parse_user_query(query)

            # æœç´¢çŸ¥è¯†åº“
            search_results = self.search_knowledge_base(parsed_query)

            # ç”Ÿæˆå“åº”
            response = self.generate_response(parsed_query, search_results)

            # è®°å½•æŸ¥è¯¢æ—¥å¿—
            self._log_query(query, parsed_query, response)

            return response

        except Exception as e:
            logger.error(f"Failed to process query: {e}")
            return {
                'status': 'error',
                'message': "å¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                'results': []
            }

    def _log_query(self, query: str, parsed_query: Dict, response: Dict):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—"""
        try:
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'query': query,
                'intent': parsed_query['intent'],
                'keywords': parsed_query['keywords'],
                'result_count': response.get('total_results', 0),
                'status': response.get('status', 'unknown')
            }

            log_file = Path("data/processed/chat_query_log.json")
            log_file.parent.mkdir(parents=True, exist_ok=True)

            # è¯»å–ç°æœ‰æ—¥å¿—
            existing_logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        existing_logs = json.load(f)
                except:
                    existing_logs = []

            # æ·»åŠ æ–°æ—¥å¿—ï¼ˆä¿ç•™æœ€è¿‘1000æ¡ï¼‰
            existing_logs.append(log_entry)
            if len(existing_logs) > 1000:
                existing_logs = existing_logs[-1000:]

            # ä¿å­˜æ—¥å¿—
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(existing_logs, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"Failed to log query: {e}")

# Initialize Flask app
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Create global chat interface instance
chat_interface = ChatInterface()

def initialize():
    """åˆå§‹åŒ–åº”ç”¨"""
    chat_interface.connect()
    chat_interface.init_embedding_search()
    logger.info("ğŸš€ Chat Interface API started successfully!")

@app.teardown_appcontext
def teardown_db(error):
    """æ¸…ç†æ•°æ®åº“è¿æ¥"""
    pass

# Chat endpoint
@app.route('/api/v1/chat/query', methods=['POST'])
def chat_query():
    """å¤„ç†èŠå¤©æŸ¥è¯¢"""
    try:
        data = request.get_json()
        if not data or 'query' not in data:
            return jsonify({
                'success': False,
                'error': 'Query is required'
            }), 400

        query = data['query'].strip()
        if not query:
            return jsonify({
                'success': False,
                'error': 'Query cannot be empty'
            }), 400

        # å¤„ç†æŸ¥è¯¢
        response = chat_interface.process_query(query)

        return jsonify({
            'success': True,
            'data': response
        })

    except Exception as e:
        logger.error(f"Chat query error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# Health check endpoint
@app.route('/api/v1/chat/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'service': 'chat-interface',
        'timestamp': datetime.now().isoformat()
    })

# Query statistics endpoint
@app.route('/api/v1/chat/stats', methods=['GET'])
def get_chat_stats():
    """è·å–èŠå¤©ç»Ÿè®¡"""
    try:
        log_file = Path("data/processed/chat_query_log.json")
        if not log_file.exists():
            return jsonify({
                'success': True,
                'data': {
                    'total_queries': 0,
                    'daily_queries': 0,
                    'popular_intents': {},
                    'success_rate': 0
                }
            })

        with open(log_file, 'r', encoding='utf-8') as f:
            logs = json.load(f)

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_queries = len(logs)
        today = datetime.now().date()
        daily_queries = len([log for log in logs if log['timestamp'].split('T')[0] == str(today)])

        # ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
        intent_counts = {}
        successful_queries = 0
        for log in logs:
            intent = log.get('intent', 'unknown')
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
            if log.get('status') == 'success':
                successful_queries += 1

        success_rate = (successful_queries / total_queries * 100) if total_queries > 0 else 0

        return jsonify({
            'success': True,
            'data': {
                'total_queries': total_queries,
                'daily_queries': daily_queries,
                'popular_intents': intent_counts,
                'success_rate': round(success_rate, 2)
            }
        })

    except Exception as e:
        logger.error(f"Failed to get chat stats: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# Error handlers
@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'success': False,
        'error': 'Endpoint not found'
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        'success': False,
        'error': 'Internal server error'
    }), 500

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Chat Interface API')
    parser.add_argument('--host', default='localhost', help='Host to bind to')
    parser.add_argument('--port', type=int, default=8002, help='Port to bind to')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')

    args = parser.parse_args()

    # åˆå§‹åŒ–åº”ç”¨
    initialize()

    logger.info(f"ğŸš€ Starting Chat Interface API on {args.host}:{args.port}")

    app.run(
        host=args.host,
        port=args.port,
        debug=args.debug,
        threaded=True
    )

if __name__ == "__main__":
    main()