#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Embedding System with LangChain Integration
Â¢ûÂº∫ÂûãÂµåÂÖ•Á≥ªÁªü‰∏éLangChainÈõÜÊàê

This module provides an enhanced embedding system that integrates with the advanced RAG system,
supporting LangChain retrievers, hierarchical document chunking, and multi-modal retrieval.
"""

import sqlite3
import json
import logging
import numpy as np
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any, Union
import os
import hashlib
import asyncio
from dataclasses import dataclass, asdict
from enum import Enum

# LangChain imports
try:
    from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
    from langchain.vectorstores import Chroma, FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
    from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever, ContextualCompressionRetriever
    from langchain.retrievers.document_compressors import LLMChainExtractor
    from langchain.schema import Document as LangChainDocument
    from langchain.embeddings import CacheBackedEmbeddings
    from langchain.storage import LocalFileStore
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    logging.warning("LangChain not available, using fallback implementations")

# Import advanced RAG components
try:
    from .core.document_chunker import DocumentChunker, ChunkingStrategy, ContentType
    from .core.multi_modal_retriever import MultiModalRetriever, RetrievalStrategy, RetrievedDocument
    from .core.citation_tracker import CitationTracker, CitationType
    from .core.database_integration import DatabaseIntegration, VectorDatabaseType
except ImportError:
    # Fallback imports if run as standalone
    import sys
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    try:
        from core.document_chunker import DocumentChunker, ChunkingStrategy, ContentType
        from core.multi_modal_retriever import MultiModalRetriever, RetrievalStrategy, RetrievedDocument
        from core.citation_tracker import CitationTracker, CitationType
        from core.database_integration import DatabaseIntegration, VectorDatabaseType
    except ImportError:
        logging.error("Advanced RAG components not available")
        DocumentChunker = None
        MultiModalRetriever = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/processed/enhanced_embeddings.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RetrievalMode(Enum):
    """Ê£ÄÁ¥¢Ê®°ÂºèÊûö‰∏æ"""
    SIMPLE = "simple"
    VECTOR = "vector"
    MULTI_MODAL = "multi_modal"
    HIERARCHICAL = "hierarchical"
    ENSEMBLE = "ensemble"
    COMPRESSION = "compression"

@dataclass
class EnhancedSearchResult:
    """Â¢ûÂº∫ÊêúÁ¥¢ÁªìÊûú"""
    entry_id: int
    content: str
    metadata: Dict[str, Any]
    score: float
    retrieval_mode: str
    chunk_info: Optional[Dict[str, Any]] = None
    citations: Optional[List[Dict[str, Any]]] = None
    context_relevance: float = 0.0

class EnhancedEmbeddingSystem:
    """Â¢ûÂº∫ÂûãÂµåÂÖ•Á≥ªÁªü"""

    def __init__(self,
                 db_path: str = "knowledge_base.db",
                 vector_db_type: VectorDatabaseType = VectorDatabaseType.CHROMA,
                 cache_dir: str = "./cache",
                 enable_langchain: bool = True):
        """
        ÂàùÂßãÂåñÂ¢ûÂº∫ÂµåÂÖ•Á≥ªÁªü

        Args:
            db_path: SQLiteÊï∞ÊçÆÂ∫ìË∑ØÂæÑ
            vector_db_type: ÂêëÈáèÊï∞ÊçÆÂ∫ìÁ±ªÂûã
            cache_dir: ÁºìÂ≠òÁõÆÂΩï
            enable_langchain: ÊòØÂê¶ÂêØÁî®LangChainÂäüËÉΩ
        """
        self.db_path = db_path
        self.vector_db_type = vector_db_type
        self.cache_dir = cache_dir
        self.enable_langchain = enable_langchain and LANGCHAIN_AVAILABLE
        self.conn = None

        # ÂàùÂßãÂåñÁªÑ‰ª∂
        self.embedding_model = None
        self.vector_store = None
        self.retriever = None
        self.document_chunker = None
        self.multi_modal_retriever = None
        self.citation_tracker = None
        self.database_integration = None

        # ÈÖçÁΩÆÂèÇÊï∞
        self.embedding_dimension = 1536
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.top_k = 10

        # ÂàõÂª∫ÁºìÂ≠òÁõÆÂΩï
        os.makedirs(cache_dir, exist_ok=True)

    async def initialize(self):
        """ÂºÇÊ≠•ÂàùÂßãÂåñÊâÄÊúâÁªÑ‰ª∂"""
        try:
            logger.info("üöÄ Initializing Enhanced Embedding System...")

            # ËøûÊé•Êï∞ÊçÆÂ∫ì
            self._connect_database()

            # ÂàùÂßãÂåñRAGÁªÑ‰ª∂
            if DocumentChunker and MultiModalRetriever:
                await self._initialize_rag_components()

            # ÂàùÂßãÂåñLangChainÁªÑ‰ª∂
            if self.enable_langchain:
                await self._initialize_langchain_components()

            # ÂàùÂßãÂåñÂêëÈáèÂ≠òÂÇ®
            await self._initialize_vector_store()

            logger.info("‚úÖ Enhanced Embedding System initialized successfully")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize enhanced embedding system: {e}")
            raise

    def _connect_database(self):
        """ËøûÊé•Êï∞ÊçÆÂ∫ì"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA foreign_keys = ON")
            logger.info(f"Connected to database: {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    async def _initialize_rag_components(self):
        """ÂàùÂßãÂåñRAGÁªÑ‰ª∂"""
        try:
            # ÂàùÂßãÂåñÊñáÊ°£ÂàÜÂùóÂô®
            self.document_chunker = DocumentChunker()
            await self.document_chunker.initialize()
            logger.info("‚úÖ Document chunker initialized")

            # ÂàùÂßãÂåñÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âô®
            self.multi_modal_retriever = MultiModalRetriever(
                db_path=self.db_path,
                vector_db_type=self.vector_db_type
            )
            await self.multi_modal_retriever.initialize()
            logger.info("‚úÖ Multi-modal retriever initialized")

            # ÂàùÂßãÂåñÂºïÁî®Ë∑üË∏™Âô®
            self.citation_tracker = CitationTracker(self.db_path)
            await self.citation_tracker.initialize()
            logger.info("‚úÖ Citation tracker initialized")

            # ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÈõÜÊàê
            self.database_integration = DatabaseIntegration(self.db_path)
            await self.database_integration.initialize()
            logger.info("‚úÖ Database integration initialized")

        except Exception as e:
            logger.error(f"Failed to initialize RAG components: {e}")
            raise

    async def _initialize_langchain_components(self):
        """ÂàùÂßãÂåñLangChainÁªÑ‰ª∂"""
        try:
            # ÂàùÂßãÂåñÂµåÂÖ•Ê®°Âûã
            await self._initialize_embeddings()

            # ÂàùÂßãÂåñÊñáÊ°£ÂàÜÂâ≤Âô®
            self._initialize_text_splitters()

            logger.info("‚úÖ LangChain components initialized")

        except Exception as e:
            logger.error(f"Failed to initialize LangChain components: {e}")
            raise

    async def _initialize_embeddings(self):
        """ÂàùÂßãÂåñÂµåÂÖ•Ê®°Âûã"""
        try:
            # Â∞ùËØïOpenAIÂµåÂÖ•
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.embedding_model = OpenAIEmbeddings(
                    model="text-embedding-3-small",
                    openai_api_key=api_key
                )
                self.embedding_dimension = 1536
                logger.info("‚úÖ Using OpenAI embeddings: text-embedding-3-small")
                return

            # Â∞ùËØïHuggingFaceÂµåÂÖ•
            self.embedding_model = HuggingFaceEmbeddings(
                model_name="all-MiniLM-L6-v2",
                cache_folder=self.cache_dir
            )
            self.embedding_dimension = 384
            logger.info("‚úÖ Using HuggingFace embeddings: all-MiniLM-L6-v2")

        except Exception as e:
            logger.warning(f"Failed to initialize advanced embeddings: {e}")
            # ‰ΩøÁî®ÁÆÄÂåñÁâàÂµåÂÖ•
            self.embedding_model = None
            logger.warning("‚ö†Ô∏è Using fallback embedding system")

    def _initialize_text_splitters(self):
        """ÂàùÂßãÂåñÊñáÊú¨ÂàÜÂâ≤Âô®"""
        try:
            # ÈÄíÂΩíÂ≠óÁ¨¶ÂàÜÂâ≤Âô®ÔºàÁî®‰∫éÂàÜÂ±ÇÊñáÊ°£Ôºâ
            self.recursive_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )

            # Â≠óÁ¨¶ÂàÜÂâ≤Âô®ÔºàÁî®‰∫éÁÆÄÂçïÊñáÊ°£Ôºâ
            self.character_splitter = CharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )

            logger.info("‚úÖ Text splitters initialized")

        except Exception as e:
            logger.error(f"Failed to initialize text splitters: {e}")

    async def _initialize_vector_store(self):
        """ÂàùÂßãÂåñÂêëÈáèÂ≠òÂÇ®"""
        try:
            if not self.embedding_model:
                logger.warning("No embedding model available, skipping vector store initialization")
                return

            # ÂàõÂª∫Êú¨Âú∞Êñá‰ª∂Â≠òÂÇ®
            store = LocalFileStore(self.cache_dir)

            # ÂàõÂª∫ÁºìÂ≠òÊîØÊåÅÁöÑÂµåÂÖ•
            cached_embeddings = CacheBackedEmbeddings(
                underlying_embeddings=self.embedding_model,
                document_embedding_cache=store
            )

            # Ê†πÊçÆÁ±ªÂûãÂàùÂßãÂåñÂêëÈáèÂ≠òÂÇ®
            vector_store_path = os.path.join(self.cache_dir, "vector_store")
            os.makedirs(vector_store_path, exist_ok=True)

            if self.vector_db_type == VectorDatabaseType.CHROMA:
                self.vector_store = Chroma(
                    embedding_function=cached_embeddings,
                    persist_directory=vector_store_path
                )
                logger.info("‚úÖ Chroma vector store initialized")

            elif self.vector_db_type == VectorDatabaseType.FAISS:
                index_path = os.path.join(vector_store_path, "faiss.index")
                if os.path.exists(index_path):
                    self.vector_store = FAISS.load_local(
                        index_path,
                        cached_embeddings,
                        "faiss.index"
                    )
                else:
                    # ÂàõÂª∫Êñ∞ÁöÑFAISSÁ¥¢Âºï
                    self.vector_store = FAISS.from_texts(
                        [""],  # Á©∫ÊñáÊ°£Áî®‰∫éÂàùÂßãÂåñ
                        cached_embeddings
                    )
                logger.info("‚úÖ FAISS vector store initialized")

            # ÂàùÂßãÂåñÊ£ÄÁ¥¢Âô®
            self._initialize_retrievers()

        except Exception as e:
            logger.error(f"Failed to initialize vector store: {e}")
            raise

    def _initialize_retrievers(self):
        """ÂàùÂßãÂåñÊ£ÄÁ¥¢Âô®"""
        try:
            if not self.vector_store:
                logger.warning("No vector store available, skipping retriever initialization")
                return

            # Âü∫Á°ÄÂêëÈáèÊ£ÄÁ¥¢Âô®
            base_retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.top_k}
            )

            # Â¶ÇÊûúÊúâÂàÜÂ±ÇÊñáÊ°£ÊîØÊåÅÔºåÂàõÂª∫Áà∂ÊñáÊ°£Ê£ÄÁ¥¢Âô®
            if self.recursive_splitter and self.character_splitter:
                try:
                    self.parent_retriever = ParentDocumentRetriever(
                        vectorstore=self.vector_store,
                        child_splitter=self.character_splitter,
                        parent_splitter=self.recursive_splitter
                    )
                    logger.info("‚úÖ Parent document retriever initialized")
                except Exception as e:
                    logger.warning(f"Failed to initialize parent retriever: {e}")

            # Âü∫Á°ÄÊ£ÄÁ¥¢Âô®
            self.retriever = base_retriever
            logger.info("‚úÖ Base retriever initialized")

        except Exception as e:
            logger.error(f"Failed to initialize retrievers: {e}")

    async def process_and_index_documents(self,
                                        chunking_strategy: ChunkingStrategy = ChunkingStrategy.HIERARCHICAL,
                                        force_reprocess: bool = False) -> Dict[str, Any]:
        """
        Â§ÑÁêÜÂπ∂Á¥¢ÂºïÊñáÊ°£

        Args:
            chunking_strategy: ÊñáÊ°£ÂàÜÂùóÁ≠ñÁï•
            force_reprocess: ÊòØÂê¶Âº∫Âà∂ÈáçÊñ∞Â§ÑÁêÜ

        Returns:
            Â§ÑÁêÜÁªìÊûúÁªüËÆ°
        """
        try:
            logger.info(f"üìö Processing documents with strategy: {chunking_strategy.value}")

            if not self.document_chunker:
                logger.error("Document chunker not initialized")
                return {"success": False, "error": "Document chunker not initialized"}

            # Ëé∑ÂèñÊâÄÊúâÁü•ËØÜÊù°ÁõÆ
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT id, entity_type, name, description, attributes_json, created_at
                FROM knowledge_entries
                ORDER BY created_at DESC
            """)

            entries = []
            for row in cursor.fetchall():
                entry = {
                    'id': row[0],
                    'entity_type': row[1],
                    'name': row[2],
                    'description': row[3],
                    'attributes_json': row[4],
                    'created_at': row[5]
                }
                entries.append(entry)

            logger.info(f"Found {len(entries)} entries to process")

            # Â§ÑÁêÜÊØè‰∏™ÊñáÊ°£
            processed_count = 0
            chunk_count = 0

            for entry in entries:
                # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÈáçÊñ∞Â§ÑÁêÜ
                if not force_reprocess:
                    cursor.execute(
                        "SELECT 1 FROM document_chunks WHERE source_id = ? LIMIT 1",
                        (entry['id'],)
                    )
                    if cursor.fetchone():
                        continue

                # ÁîüÊàêÊñáÊ°£ÂÜÖÂÆπ
                content = self._generate_document_content(entry)
                if not content.strip():
                    continue

                # ÂàõÂª∫ÂÖÉÊï∞ÊçÆ
                metadata = {
                    'source_id': entry['id'],
                    'entity_type': entry['entity_type'],
                    'title': entry['name'],
                    'created_at': entry['created_at']
                }

                # Â§ÑÁêÜÊñáÊ°£ÂàÜÂùó
                chunks = await self.document_chunker.chunk_document(
                    doc_id=str(entry['id']),
                    content=content,
                    content_type=ContentType.TEXT,
                    metadata=metadata,
                    strategy=chunking_strategy
                )

                # ‰ΩøÁî®LangChainÂ§ÑÁêÜÂàÜÂùóÔºàÂ¶ÇÊûúÂèØÁî®Ôºâ
                if self.enable_langchain and chunks:
                    await self._process_chunks_with_langchain(chunks, entry)

                processed_count += 1
                chunk_count += len(chunks)

                if processed_count % 10 == 0:
                    logger.info(f"Processed {processed_count} entries, {chunk_count} chunks")

            # ‰øùÂ≠òÂêëÈáèÂ≠òÂÇ®
            if self.vector_store and self.vector_db_type == VectorDatabaseType.CHROMA:
                self.vector_store.persist()
            elif self.vector_store and self.vector_db_type == VectorDatabaseType.FAISS:
                vector_store_path = os.path.join(self.cache_dir, "vector_store")
                self.vector_store.save_local(vector_store_path, "faiss.index")

            logger.info(f"‚úÖ Document processing completed")
            logger.info(f"üìä Processed: {processed_count} entries")
            logger.info(f"üß© Generated: {chunk_count} chunks")

            return {
                "success": True,
                "processed_entries": processed_count,
                "generated_chunks": chunk_count,
                "strategy_used": chunking_strategy.value
            }

        except Exception as e:
            logger.error(f"Failed to process documents: {e}")
            return {"success": False, "error": str(e)}

    async def _process_chunks_with_langchain(self, chunks: List[Any], entry: Dict):
        """‰ΩøÁî®LangChainÂ§ÑÁêÜÊñáÊ°£ÂàÜÂùó"""
        try:
            if not self.embedding_model:
                return

            # ËΩ¨Êç¢‰∏∫LangChainÊñáÊ°£Ê†ºÂºè
            langchain_docs = []
            for chunk in chunks:
                metadata = {
                    'source_id': chunk.source_id,
                    'chunk_id': chunk.chunk_id,
                    'entity_type': entry['entity_type'],
                    'title': entry['name'],
                    'chunk_index': chunk.chunk_index,
                    'content_type': chunk.content_type.value,
                    **chunk.metadata
                }

                doc = LangChainDocument(
                    page_content=chunk.content,
                    metadata=metadata
                )
                langchain_docs.append(doc)

            # ÊâπÈáèÊ∑ªÂä†Âà∞ÂêëÈáèÂ≠òÂÇ®
            if langchain_docs:
                if self.vector_db_type == VectorDatabaseType.FAISS:
                    self.vector_store.add_documents(langchain_docs)
                else:
                    # ChromaÊîØÊåÅÊâπÈáèÊ∑ªÂä†
                    self.vector_store.add_documents(langchain_docs)

        except Exception as e:
            logger.error(f"Failed to process chunks with LangChain: {e}")

    def _generate_document_content(self, entry: Dict) -> str:
        """ÁîüÊàêÊñáÊ°£ÂÜÖÂÆπ"""
        content_parts = []

        if entry.get('name'):
            content_parts.append(f"Ê†áÈ¢ò: {entry['name']}")

        if entry.get('description'):
            content_parts.append(f"ÊèèËø∞: {entry['description']}")

        if entry.get('attributes_json'):
            try:
                attributes = json.loads(entry['attributes_json'])
                for key, value in attributes.items():
                    if value:
                        content_parts.append(f"{key}: {value}")
            except json.JSONDecodeError:
                pass

        if entry.get('entity_type'):
            content_parts.append(f"Á±ªÂûã: {entry['entity_type']}")

        return "\n".join(content_parts)

    async def search(self,
                    query: str,
                    mode: RetrievalMode = RetrievalMode.MULTI_MODAL,
                    top_k: int = None,
                    content_types: Optional[List[str]] = None,
                    filters: Optional[Dict[str, Any]] = None) -> List[EnhancedSearchResult]:
        """
        Â¢ûÂº∫ÊêúÁ¥¢ÂäüËÉΩ

        Args:
            query: ÊêúÁ¥¢Êü•ËØ¢
            mode: Ê£ÄÁ¥¢Ê®°Âºè
            top_k: ËøîÂõûÁªìÊûúÊï∞Èáè
            content_types: ÂÜÖÂÆπÁ±ªÂûãËøáÊª§
            filters: ËøáÊª§Êù°‰ª∂

        Returns:
            Â¢ûÂº∫ÊêúÁ¥¢ÁªìÊûúÂàóË°®
        """
        try:
            top_k = top_k or self.top_k

            if mode == RetrievalMode.MULTI_MODAL and self.multi_modal_retriever:
                return await self._multi_modal_search(query, top_k, content_types, filters)

            elif mode == RetrievalMode.HIERARCHICAL and hasattr(self, 'parent_retriever'):
                return await self._hierarchical_search(query, top_k, filters)

            elif mode == RetrievalMode.ENSEMBLE and self.enable_langchain:
                return await self._ensemble_search(query, top_k, filters)

            elif mode == RetrievalMode.COMPRESSION and self.enable_langchain:
                return await self._compression_search(query, top_k, filters)

            else:
                # ÈªòËÆ§ÂêëÈáèÊêúÁ¥¢
                return await self._vector_search(query, top_k, filters)

        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    async def _multi_modal_search(self, query: str, top_k: int, content_types: Optional[List[str]], filters: Optional[Dict[str, Any]]) -> List[EnhancedSearchResult]:
        """Â§öÊ®°ÊÄÅÊêúÁ¥¢"""
        try:
            if not self.multi_modal_retriever:
                return []

            # ËΩ¨Êç¢ÂÜÖÂÆπÁ±ªÂûã
            content_type_enums = []
            if content_types:
                for ct in content_types:
                    try:
                        content_type_enums.append(ContentType(ct))
                    except ValueError:
                        continue

            # ÊâßË°åÊ£ÄÁ¥¢
            results = await self.multi_modal_retriever.retrieve(
                query=query,
                content_types=content_type_enums or None,
                top_k=top_k,
                strategy=RetrievalStrategy.MULTI_MODAL,
                filters=filters or {}
            )

            # ËΩ¨Êç¢ÁªìÊûúÊ†ºÂºè
            enhanced_results = []
            for result in results:
                enhanced_result = EnhancedSearchResult(
                    entry_id=int(result.document.chunk_id.split('_')[0]) if '_' in result.document.chunk_id else 0,
                    content=result.document.content,
                    metadata=result.document.metadata,
                    score=result.relevance_score,
                    retrieval_mode="multi_modal",
                    chunk_info={
                        'chunk_id': result.document.chunk_id,
                        'content_type': result.document.content_type.value,
                        'section': result.document.metadata.get('section', ''),
                        'keywords': result.document.metadata.get('keywords', [])
                    }
                )
                enhanced_results.append(enhanced_result)

            return enhanced_results

        except Exception as e:
            logger.error(f"Multi-modal search failed: {e}")
            return []

    async def _vector_search(self, query: str, top_k: int, filters: Optional[Dict[str, Any]]) -> List[EnhancedSearchResult]:
        """ÂêëÈáèÊêúÁ¥¢"""
        try:
            if not self.retriever:
                return []

            # Ê£ÄÁ¥¢ÊñáÊ°£
            docs = self.retriever.get_relevant_documents(query)

            # ËΩ¨Êç¢ÁªìÊûú
            enhanced_results = []
            for i, doc in enumerate(docs[:top_k]):
                enhanced_result = EnhancedSearchResult(
                    entry_id=doc.metadata.get('source_id', 0),
                    content=doc.page_content,
                    metadata=doc.metadata,
                    score=1.0 - (i * 0.1),  # ÁÆÄÂçïÁöÑÈÄíÂáèÂàÜÊï∞
                    retrieval_mode="vector"
                )
                enhanced_results.append(enhanced_result)

            return enhanced_results

        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []

    async def _hierarchical_search(self, query: str, top_k: int, filters: Optional[Dict[str, Any]]) -> List[EnhancedSearchResult]:
        """ÂàÜÂ±ÇÊêúÁ¥¢"""
        try:
            if not hasattr(self, 'parent_retriever'):
                return await self._vector_search(query, top_k, filters)

            docs = self.parent_retriever.get_relevant_documents(query)

            enhanced_results = []
            for i, doc in enumerate(docs[:top_k]):
                enhanced_result = EnhancedSearchResult(
                    entry_id=doc.metadata.get('source_id', 0),
                    content=doc.page_content,
                    metadata=doc.metadata,
                    score=1.0 - (i * 0.1),
                    retrieval_mode="hierarchical"
                )
                enhanced_results.append(enhanced_result)

            return enhanced_results

        except Exception as e:
            logger.error(f"Hierarchical search failed: {e}")
            return []

    async def _ensemble_search(self, query: str, top_k: int, filters: Optional[Dict[str, Any]]) -> List[EnhancedSearchResult]:
        """ÈõÜÊàêÊêúÁ¥¢"""
        try:
            # ÁªìÂêàÂ§öÁßçÊ£ÄÁ¥¢Á≠ñÁï•
            vector_results = await self._vector_search(query, top_k, filters)

            # Â¶ÇÊûúÊúâÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âô®Ôºå‰πü‰ΩøÁî®ÂÆÉ
            if self.multi_modal_retriever:
                multi_modal_results = await self._multi_modal_search(query, top_k, None, filters)
                # ÂêàÂπ∂ÂíåÈáçÊñ∞ÊéíÂ∫èÁªìÊûú
                all_results = vector_results + multi_modal_results
                # ÁÆÄÂçïÁöÑÂéªÈáçÂíåÈáçÊéíÂ∫è
                unique_results = {}
                for result in all_results:
                    key = f"{result.entry_id}_{result.content[:100]}"
                    if key not in unique_results or result.score > unique_results[key].score:
                        unique_results[key] = result

                results = list(unique_results.values())
                results.sort(key=lambda x: x.score, reverse=True)
                return results[:top_k]

            return vector_results

        except Exception as e:
            logger.error(f"Ensemble search failed: {e}")
            return []

    async def _compression_search(self, query: str, top_k: int, filters: Optional[Dict[str, Any]]) -> List[EnhancedSearchResult]:
        """ÂéãÁº©ÊêúÁ¥¢ÔºàÂéãÁº©Ê£ÄÁ¥¢ÁªìÊûúÔºâ"""
        try:
            # ÂÖàËøõË°åÂü∫Á°ÄÊ£ÄÁ¥¢
            base_results = await self._vector_search(query, top_k * 2, filters)

            # ËøôÈáåÂèØ‰ª•Ê∑ªÂä†ÂÜÖÂÆπÂéãÁº©ÈÄªËæë
            # ÁÆÄÂåñÁâàÊú¨ÔºöÁõ¥Êé•ËøîÂõûÂü∫Á°ÄÁªìÊûú
            return base_results[:top_k]

        except Exception as e:
            logger.error(f"Compression search failed: {e}")
            return []

    def get_system_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁ≥ªÁªüÁªüËÆ°‰ø°ÊÅØ"""
        try:
            stats = {
                'enable_langchain': self.enable_langchain,
                'vector_db_type': self.vector_db_type.value,
                'embedding_dimension': self.embedding_dimension,
                'chunk_size': self.chunk_size,
                'top_k': self.top_k,
                'components_initialized': {
                    'document_chunker': self.document_chunker is not None,
                    'multi_modal_retriever': self.multi_modal_retriever is not None,
                    'citation_tracker': self.citation_tracker is not None,
                    'database_integration': self.database_integration is not None,
                    'embedding_model': self.embedding_model is not None,
                    'vector_store': self.vector_store is not None,
                    'retriever': self.retriever is not None
                }
            }

            # Ëé∑ÂèñÂêëÈáèÂ≠òÂÇ®ÁªüËÆ°
            if self.vector_store:
                try:
                    if hasattr(self.vector_store, '_collection'):
                        # Chroma
                        stats['vector_store_stats'] = {
                            'type': 'Chroma',
                            'document_count': self.vector_store._collection.count()
                        }
                    elif hasattr(self.vector_store, 'index'):
                        # FAISS
                        stats['vector_store_stats'] = {
                            'type': 'FAISS',
                            'dimension': self.vector_store.index.d if hasattr(self.vector_store.index, 'd') else 'unknown'
                        }
                except Exception as e:
                    logger.warning(f"Failed to get vector store stats: {e}")

            return stats

        except Exception as e:
            logger.error(f"Failed to get system stats: {e}")
            return {'error': str(e)}

    def close(self):
        """ÂÖ≥Èó≠Á≥ªÁªü"""
        try:
            if self.conn:
                self.conn.close()
                logger.info("Database connection closed")

            # ‰øùÂ≠òÂêëÈáèÂ≠òÂÇ®
            if self.vector_store:
                if self.vector_db_type == VectorDatabaseType.CHROMA:
                    self.vector_store.persist()
                elif self.vector_db_type == VectorDatabaseType.FAISS:
                    vector_store_path = os.path.join(self.cache_dir, "vector_store")
                    self.vector_store.save_local(vector_store_path, "faiss.index")

                logger.info("Vector store saved")

        except Exception as e:
            logger.error(f"Error closing system: {e}")

async def main():
    """‰∏ªÂáΩÊï∞"""
    import argparse

    parser = argparse.ArgumentParser(description='Enhanced Embedding System')
    parser.add_argument('--init', action='store_true', help='Initialize system')
    parser.add_argument('--process', action='store_true', help='Process and index documents')
    parser.add_argument('--search', type=str, help='Search query')
    parser.add_argument('--mode', type=str, default='multi_modal',
                       choices=['simple', 'vector', 'multi_modal', 'hierarchical', 'ensemble', 'compression'],
                       help='Search mode')
    parser.add_argument('--top-k', type=int, default=5, help='Number of results')
    parser.add_argument('--stats', action='store_true', help='Show system statistics')
    parser.add_argument('--force', action='store_true', help='Force reprocess')

    args = parser.parse_args()

    # ÂàõÂª∫Á≥ªÁªüÂÆû‰æã
    system = EnhancedEmbeddingSystem()

    try:
        if args.init or args.process or args.search or args.stats:
            await system.initialize()

        if args.process:
            result = await system.process_and_index_documents(force_reprocess=args.force)
            print(f"Processing result: {result}")

        elif args.search:
            mode = RetrievalMode(args.mode)
            results = await system.search(args.search, mode=mode, top_k=args.top_k)

            print(f"\nüîç Search Results for: '{args.search}' (Mode: {args.mode})")
            print("=" * 80)

            if results:
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. [Score: {result.score:.3f}] {result.retrieval_mode.upper()}")
                    print(f"   Entry ID: {result.entry_id}")
                    print(f"   Content: {result.content[:200]}...")
                    if result.chunk_info:
                        print(f"   Chunk: {result.chunk_info}")
            else:
                print("No results found.")

        elif args.stats:
            stats = system.get_system_stats()
            print("\nüìä Enhanced Embedding System Statistics")
            print("=" * 50)
            print(json.dumps(stats, indent=2, ensure_ascii=False))

        else:
            parser.print_help()

    except Exception as e:
        logger.error(f"Error: {e}")
        import traceback
        traceback.print_exc()

    finally:
        system.close()

if __name__ == "__main__":
    asyncio.run(main())