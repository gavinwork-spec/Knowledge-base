#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Document Parser Script
æ–‡æ¡£è§£æè„šæœ¬

This script parses various document formats (PDF, Excel, Word) to extract
structured knowledge information and store it in the knowledge database.
"""

import sqlite3
import json
import logging
import argparse
import os
import re
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/processed/parse_documents.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DocumentParser:
    """æ–‡æ¡£è§£æå™¨"""

    def __init__(self, db_path: str = "knowledge_base.db"):
        self.db_path = db_path
        self.conn = None
        self.parsing_rules = self.load_parsing_rules()
        self.processed_files = set()

    def load_parsing_rules(self) -> Dict:
        """åŠ è½½è§£æè§„åˆ™"""
        return {
            'customer_extraction': {
                'patterns': {
                    'company_name': [
                        r'(?:)(å…¬å¸|æœ‰é™å…¬å¸|è‚¡ä»½å…¬å¸|ä¼ä¸š|å®ä¸š|ç§‘æŠ€|åˆ¶é€ )',
                        r'[A-Za-z][\w\u4e00-\u9fa5]+(?:å…¬å¸|æœ‰é™å…¬å¸|è‚¡ä»½å…¬å¸|ä¼ä¸š|å®ä¸š|ç§‘æŠ€|åˆ¶é€ )',
                    ],
                    'contact_person': [
                        r'(?:)(ç»ç†|ä¸»ç®¡|è´Ÿè´£äºº|å…ˆç”Ÿ|å¥³å£«)\s*[A-Za-z\u4e00-\u9fa5]+',
                    ],
                    'phone_number': [
                        r'1[3-9]\d{9}',
                        r'\d{3}-\d{4}-\d{4}',
                        r'\d{11}',
                    ],
                    'email': [
                        r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
                    ]
                },
                'confidence_threshold': 0.7,
                'context_window': 50
            },
            'product_extraction': {
                'patterns': {
                    'product_name': [
                        r'(?:)(èºæ “|èºé’‰|èºä¸|ç´§å›ºä»¶|å«ç‰‡|è½´æ‰¿)',
                        r'[A-Z]+-\d+',
                        r'M\d+',
                    ],
                    'specification': [
                        r'M\d+',
                        r'\d+\s*mm',
                        r'Ï†\d+',
                        r'\d+\s*Ã—\s*\d+',
                    ],
                    'material': [
                        r'(?:)(ä¸é”ˆé’¢|ç¢³é’¢|åˆé‡‘é’¢|é“œ|é“|é”Œ|å°¼é¾™|å¡‘æ–™)',
                        r'SS\d+',
                        r'Q\d+',
                        r'45#',
                        r'304',
                        r'316',
                    ],
                    'quantity': [
                        r'\d+\s*(?:ä¸ª|ä»¶|å¥—|ç®±|åŒ…)',
                        r'(?:)quantity\s*[ï¼š:]?\s*\d+',
                    ],
                    'unit_price': [
                        r'ï¿¥\s*\d+\.?\d*',
                        r'RMB\s*\d+\.?\d*',
                        r'\$\s*\d+\.?\d*',
                    ]
                },
                'confidence_threshold': 0.7,
                'context_window': 30
            }
        }

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
            logger.info(f"Connected to database: {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")

    def calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""

    def is_file_processed(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_hash = self.calculate_file_hash(file_path)
        return file_hash in self.processed_files

    def mark_file_processed(self, file_path: str):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_hash = self.calculate_file_hash(file_path)
        self.processed_files.add(file_hash)

    def extract_text_from_pdf(self, file_path: str) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            import pdfplumber

            text = ""
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        text += page_text + "\n"
                        logger.debug(f"Extracted text from page {page_num + 1}")
                    except Exception as e:
                        logger.warning(f"Failed to extract text from page {page_num + 1}: {e}")

            return text
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {file_path}: {e}")
            return ""

    def extract_data_from_excel(self, file_path: str) -> List[Dict]:
        """ä»Excelæ–‡ä»¶æå–æ•°æ®"""
        try:
            import pandas as pd

            data = []

            # è¯»å–Excelæ–‡ä»¶
            if file_path.endswith('.xls'):
                df = pd.read_excel(file_path)
            else:
                df = pd.read_excel(file_path, engine='openpyxl')

            # å°†æ•°æ®è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            for index, row in df.iterrows():
                row_dict = {}
                for col in df.columns:
                    if pd.notna(row[col]):
                        row_dict[col] = str(row[col])
                data.append(row_dict)

            logger.info(f"Extracted {len(data)} rows from Excel file")
            return data

        except Exception as e:
            logger.error(f"Failed to extract data from Excel {file_path}: {e}")
            return []

    def extract_text_from_docx(self, file_path: str) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        try:
            from docx import Document

            doc = Document(file_path)
            text = ""

            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"

            return text
        except Exception as e:
            logger.error(f"Failed to extract text from DOCX {file_path}: {e}")
            return ""

    def extract_entities(self, text: str, file_info: Dict) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        entities = []

        try:
            # å®¢æˆ·ä¿¡æ¯æå–
            if self.parsing_rules.get('customer_extraction'):
                customer_entities = self.extract_entities_by_rule(
                    text, self.parsing_rules['customer_extraction'], 'customer', file_info
                )
                entities.extend(customer_entities)

            # äº§å“ä¿¡æ¯æå–
            if self.parsing_rules.get('product_extraction'):
                product_entities = self.extract_entities_by_rule(
                    text, self.parsing_rules['product_extraction'], 'product', file_info
                )
                entities.extend(product_entities)

        except Exception as e:
            logger.error(f"Failed to extract entities: {e}")

        return entities

    def extract_entities_by_rule(self, text: str, rule: Dict, entity_type: str, file_info: Dict) -> List[Dict]:
        """æ ¹æ®è§„åˆ™æå–å®ä½“"""
        entities = []
        patterns = rule.get('patterns', {})
        threshold = rule.get('confidence_threshold', 0.7)

        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                try:
                    matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
                    for match in matches:
                        entity = {
                            'category': category,
                            'keyword': match.group(0),
                            'value': match.group(0),
                            'confidence_score': self.calculate_confidence(match, text, rule),
                            'context_text': text[max(0, match.start()-50):match.end()+50],
                            'start_position': match.start(),
                            'end_position': match.end(),
                            'file_path': file_info['path'],
                            'file_name': file_info['name'],
                            'extracted_at': datetime.now().isoformat()
                        }

                        if entity['confidence_score'] >= threshold:
                            entities.append(entity)

                except Exception as e:
                    logger.warning(f"Error in pattern matching for {category}: {e}")

        return entities

    def calculate_confidence(self, match, full_text: str, rule: Dict) -> float:
        """è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦"""
        try:
            # åŸºç¡€ç½®ä¿¡åº¦
            base_confidence = 0.8

            # æ ¹æ®åŒ¹é…é•¿åº¦è°ƒæ•´
            match_length = len(match.group(0))
            if match_length >= 10:
                length_bonus = 0.1
            elif match_length >= 5:
                length_bonus = 0.05
            else:
                length_bonus = 0.0

            # æ ¹æ®ä¸Šä¸‹æ–‡ç›¸å…³æ€§è°ƒæ•´
            context_window = rule.get('context_window', 50)
            start_pos = max(0, match.start() - context_window)
            end_pos = min(len(full_text), match.end() + context_window)
            context = full_text[start_pos:end_pos]

            # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦æœ‰ç›¸å…³å…³é”®è¯
            relevant_keywords = ['æŠ¥ä»·', 'å®¢æˆ·', 'äº§å“', 'è§„æ ¼', 'ä»·æ ¼', 'è®¢å•', 'åˆåŒ']
            context_relevance = sum(1 for keyword in relevant_keywords if keyword.lower() in context.lower())

            if context_relevance > 0:
                context_bonus = min(0.2, context_relevance * 0.05)
            else:
                context_bonus = 0.0

            confidence = min(1.0, base_confidence + length_bonus + context_bonus)
            return confidence

        except Exception as e:
            logger.warning(f"Error calculating confidence: {e}")
            return 0.5

    def create_knowledge_entry(self, file_info: Dict, entities: List[Dict]) -> Dict:
        """åˆ›å»ºçŸ¥è¯†æ¡ç›®"""
        try:
            # ç¡®å®šå®ä½“ç±»å‹
            entity_type = self.determine_entity_type(file_info, entities)

            # æå–ä¸»è¦ä¿¡æ¯
            name = self.extract_main_name(file_info, entities)
            description = self.generate_description(file_info, entities)

            # æ„å»ºå±æ€§æ•°æ®
            attributes = {
                'file_path': file_info['path'],
                'file_name': file_info['name'],
                'file_size': file_info.get('size', 0),
                'file_modified': file_info.get('modified', ''),
                'extraction_date': datetime.now().isoformat(),
                'entity_count': len(entities),
                'entities': entities
            }

            # æ·»åŠ æå–çš„å®ä½“ä¿¡æ¯
            if entities:
                for entity in entities:
                    if entity['category'] not in attributes:
                        attributes[entity['category']] = []
                    attributes[entity['category']].append(entity['value'])

            knowledge_entry = {
                'entity_type': entity_type,
                'name': name,
                'related_file': file_info['path'],
                'description': description,
                'attributes_json': json.dumps(attributes, ensure_ascii=False),
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }

            return knowledge_entry

        except Exception as e:
            logger.error(f"Failed to create knowledge entry: {e}")
            return {}

    def determine_entity_type(self, file_info: Dict, entities: List[Dict]) -> str:
        """ç¡®å®šå®ä½“ç±»å‹"""
        file_name = file_info.get('name', '').lower()
        file_path = file_info.get('path', '').lower()

        # æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šç±»å‹
        if 'customer' in file_path or 'å®¢æˆ·' in file_path:
            return 'customer'
        elif 'quote' in file_path or 'æŠ¥ä»·' in file_path or 'inquiry' in file_path:
            return 'quote'
        elif 'drawing' in file_path or 'å›¾çº¸' in file_path:
            return 'drawing'
        elif 'factory' in file_path or 'å·¥å‚' in file_path:
            return 'factory'
        elif entities:
            # æ ¹æ®æå–çš„å®ä½“ç¡®å®šç±»å‹
            if any(e['category'] in ['company_name', 'contact_person'] for e in entities):
                return 'customer'
            elif any(e['category'] in ['product_name', 'specification', 'material'] for e in entities):
                return 'product'
            elif any(e['category'] in ['unit_price', 'total_price'] for e in entities):
                return 'quote'
            else:
                return 'general'
        else:
            return 'document'

    def extract_main_name(self, file_info: Dict, entities: List[Dict]) -> str:
        """æå–ä¸»è¦åç§°"""
        # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶å
        file_name = file_info.get('name', '')
        if file_name:
            # ç§»é™¤æ–‡ä»¶æ‰©å±•å
            name_without_ext = os.path.splitext(file_name)[0]
            return name_without_ext

        # ä»å®ä½“ä¸­æå–åç§°
        if entities:
            # ä¼˜å…ˆä½¿ç”¨å…¬å¸åç§°
            for entity in entities:
                if entity['category'] == 'company_name':
                    return entity['value']

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªé«˜ç½®ä¿¡åº¦çš„å®ä½“
            high_confidence_entities = [e for e in entities if e['confidence_score'] >= 0.8]
            if high_confidence_entities:
                return high_confidence_entities[0]['value']

        return file_name or "æœªçŸ¥æ–‡æ¡£"

    def generate_description(self, file_info: Dict, entities: List[Dict]) -> str:
        """ç”Ÿæˆæè¿°"""
        description_parts = []

        # æ·»åŠ æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        if file_info.get('name'):
            description_parts.append(f"æ–‡æ¡£åç§°: {file_info['name']}")

        # æ·»åŠ æå–çš„å®ä½“ä¿¡æ¯
        if entities:
            entity_summary = {}
            for entity in entities:
                category = entity['category']
                if category not in entity_summary:
                    entity_summary[category] = []
                entity_summary[category].append(entity['value'])

            for category, values in entity_summary.items():
                if len(values) > 3:
                    description_parts.append(f"{category}: {', '.join(values[:3])} ç­‰{len(values)-3}ä¸ª")
                else:
                    description_parts.append(f"{category}: {', '.join(values)}")

        # æ·»åŠ æå–æ—¶é—´
        description_parts.append(f"æå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        return "; ".join(description_parts)

    def save_knowledge_entry(self, entry: Dict) -> Optional[int]:
        """ä¿å­˜çŸ¥è¯†æ¡ç›®åˆ°æ•°æ®åº“"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO knowledge_entries (
                    entity_type, name, related_file, description, attributes_json,
                    created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                entry['entity_type'],
                entry['name'],
                entry['related_file'],
                entry['description'],
                entry['attributes_json'],
                entry['created_at'],
                entry['updated_at']
            ))

            entry_id = cursor.lastrowid
            self.conn.commit()

            # ä¿å­˜NLPå®ä½“
            self.save_nlp_entities(entry_id, entry.get('attributes_json', '{}').get('entities', []))

            logger.info(f"Saved knowledge entry: {entry['name']} (ID: {entry_id})")
            return entry_id

        except Exception as e:
            logger.error(f"Failed to save knowledge entry: {e}")
            return None

    def save_nlp_entities(self, entry_id: int, entities: List[Dict]):
        """ä¿å­˜NLPå®ä½“åˆ°æ•°æ®åº“"""
        try:
            cursor = self.conn.cursor()

            for entity in entities:
                cursor.execute("""
                    INSERT INTO nlp_entities (
                        entry_id, keyword, value, category, confidence_score,
                        context_text, start_position, end_position, created_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    entry_id,
                    entity['keyword'],
                    entity['value'],
                    entity['category'],
                    entity['confidence_score'],
                    entity['context_text'],
                    entity['start_position'],
                    entity['end_position'],
                    datetime.now()
                ))

            self.conn.commit()
            logger.info(f"Saved {len(entities)} NLP entities for entry {entry_id}")

        except Exception as e:
            logger.error(f"Failed to save NLP entities: {e}")

    def parse_document(self, file_path: str) -> Optional[Dict]:
        """è§£æå•ä¸ªæ–‡æ¡£"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                logger.error(f"File not found: {file_path}")
                return None

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†
            if self.is_file_processed(str(file_path)):
                logger.info(f"File already processed: {file_path}")
                return None

            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = {
                'path': str(file_path),
                'name': file_path.name,
                'size': file_path.stat().st_size,
                'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                'extension': file_path.suffix.lower()
            }

            logger.info(f"Processing document: {file_info['name']}")

            # æå–æ–‡æœ¬å†…å®¹
            text = ""
            if file_info['extension'] == '.pdf':
                text = self.extract_text_from_pdf(str(file_path))
            elif file_info['extension'] in ['.xlsx', '.xls']:
                # Excelæ–‡ä»¶éœ€è¦ç‰¹æ®Šå¤„ç†
                excel_data = self.extract_data_from_excel(str(file_path))
                if excel_data:
                    text = "\n".join([str(row) for row in excel_data[:5]])  # åªå–å‰5è¡Œç”¨äºå®ä½“æå–
            elif file_info['extension'] in ['.docx', '.doc']:
                text = self.extract_text_from_docx(str(file_path))
            elif file_info['extension'] in ['.txt', '.csv']:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                except UnicodeDecodeError:
                    with open(file_path, 'r', encoding='gbk') as f:
                        text = f.read()
            else:
                logger.warning(f"Unsupported file format: {file_info['extension']}")
                return None

            if not text.strip():
                logger.warning(f"No text extracted from: {file_info['name']}")
                return None

            # æå–å®ä½“
            entities = self.extract_entities(text, file_info)

            if not entities:
                logger.warning(f"No entities extracted from: {file_info['name']}")
                return None

            # åˆ›å»ºçŸ¥è¯†æ¡ç›®
            knowledge_entry = self.create_knowledge_entry(file_info, entities)

            # ä¿å­˜åˆ°æ•°æ®åº“
            entry_id = self.save_knowledge_entry(knowledge_entry)

            if entry_id:
                # æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†
                self.mark_file_processed(str(file_path))

                # ä¿å­˜è§£æç»“æœ
                self.save_parsing_result(file_info, entities, entry_id)

                return {
                    'success': True,
                    'entry_id': entry_id,
                    'entity_count': len(entities),
                    'file_info': file_info
                }
            else:
                return {'success': False, 'error': 'Failed to save knowledge entry'}

        except Exception as e:
            logger.error(f"Failed to parse document {file_path}: {e}")
            return {'success': False, 'error': str(e)}

    def save_parsing_result(self, file_info: Dict, entities: List[Dict], entry_id: int):
        """ä¿å­˜è§£æç»“æœ"""
        try:
            os.makedirs('data/processed', exist_ok=True)

            result = {
                'file_info': file_info,
                'parsing_timestamp': datetime.now().isoformat(),
                'entry_id': entry_id,
                'entity_count': len(entities),
                'entities': entities,
                'success': True
            }

            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            date_str = datetime.now().strftime('%Y%m%d')
            filename = f"parsed_docs_{date_str}.json"

            existing_results = []
            json_file = Path('data/processed') / filename

            if json_file.exists():
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        existing_results = json.load(f)
                except:
                    existing_results = []

            existing_results.append(result)

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(existing_results, f, ensure_ascii=False, indent=2)

            logger.info(f"Saved parsing result to {filename}")

        except Exception as e:
            logger.error(f"Failed to save parsing result: {e}")

    def parse_directory(self, directory: str, recursive: bool = True) -> Dict:
        """è§£æç›®å½•ä¸­çš„æ–‡æ¡£"""
        directory_path = Path(directory)
        if not directory_path.exists():
            logger.error(f"Directory not found: {directory}")
            return {'success': False, 'error': 'Directory not found'}

        results = {
            'directory': str(directory_path),
            'start_time': datetime.now().isoformat(),
            'files_processed': 0,
            'entries_created': 0,
            'files_failed': 0,
            'errors': [],
            'processed_files': []
        }

        # æŸ¥æ‰¾æ”¯æŒçš„æ–‡ä»¶
        supported_extensions = ['.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.csv']

        if recursive:
            files = list(directory_path.rglob('*'))
        else:
            files = list(directory_path.glob('*'))

        files = [f for f in files if f.suffix.lower() in supported_extensions]

        logger.info(f"Found {len(files)} supported files in {directory}")

        for file_path in files:
            try:
                result = self.parse_document(str(file_path))
                if result:
                    if result['success']:
                        results['files_processed'] += 1
                        results['entries_created'] += 1
                        results['processed_files'].append({
                            'file': str(file_path),
                            'entry_id': result.get('entry_id'),
                            'entity_count': result.get('entity_count', 0)
                        })
                    else:
                        results['files_failed'] += 1
                        results['errors'].append({
                            'file': str(file_path),
                            'error': result.get('error', 'Unknown error')
                        })
                else:
                    results['files_failed'] += 1
                    results['errors'].append({
                        'file': str(file_path),
                        'error': 'No result returned'
                    })

            except Exception as e:
                results['files_failed'] += 1
                results['errors'].append({
                    'file': str(file_path),
                    'error': str(e)
                })

        results['end_time'] = datetime.now().isoformat()
        results['duration_seconds'] = (
            datetime.fromisoformat(results['end_time']) -
            datetime.fromisoformat(results['start_time'])
        ).total_seconds()

        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        self.save_batch_results(results)

        logger.info(f"Directory parsing completed: {results['files_processed']} processed, {results['files_failed']} failed")

        return results

    def save_batch_results(self, results: Dict):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            os.makedirs('data/processed', exist_ok=True)

            filename = f"batch_parsing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            output_path = Path('data/processed') / filename

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            logger.info(f"Saved batch results to {filename}")

        except Exception as e:
            logger.error(f"Failed to save batch results: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Document parsing tool')
    parser.add_argument('--mode', choices=['single', 'batch'], default='batch',
                       help='Parsing mode: single file or directory')
    parser.add_argument('--file', type=str, help='File path (for single mode)')
    parser.add_argument('--directory', type=str, help='Directory path (for batch mode)')
    parser.add_argument('--recursive', action='store_true', default=True,
                       help='Search directories recursively')
    parser.add_argument('--db-path', type=str, default='knowledge_base.db',
                       help='Database file path')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        parser = DocumentParser(args.db_path)
        parser.connect()

        if args.mode == 'single':
            if not args.file:
                print("Error: --file argument is required for single mode")
                return 1

            result = parser.parse_document(args.file)
            if result and result.get('success'):
                print(f"âœ… Successfully parsed document: {args.file}")
                print(f"   Entry ID: {result.get('entry_id')}")
                print(f"   Entities extracted: {result.get('entity_count', 0)}")
            else:
                print(f"âŒ Failed to parse document: {args.file}")
                print(f"   Error: {result.get('error', 'Unknown error')}")
                return 1

        elif args.mode == 'batch':
            directory = args.directory or "/Users/gavin/Nutstore Files/.symlinks/åšæœäº‘/005-è¯¢ç›˜è¯¢ä»·å’Œ/"
            result = parser.parse_directory(directory, args.recursive)

            print(f"\nğŸ“Š Parsing Results:")
            print(f"   Directory: {result['directory']}")
            print(f"   Files processed: {result['files_processed']}")
            print(f"   Entries created: {result['entries_created']}")
            print(f"   Files failed: {result['files_failed']}")
            print(f"   Duration: {results['duration_seconds']:.2f} seconds")

            if result['errors']:
                print(f"\nâŒ Errors ({len(result['errors'])}):")
                for error in result['errors']:
                    print(f"   â€¢ {error['file']}: {error['error']}")
            else:
                print("\nâœ… All files processed successfully!")

        parser.close()
        return 0

    except Exception as e:
        logger.error(f"Document parsing failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())