#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Learn From Updates - è‡ªåŠ¨å­¦ä¹ è„šæœ¬
æ ¹æ®æ–°å¢æ–‡ä»¶å’Œæé†’è®°å½•æ›´æ–°çŸ¥è¯†åº“

This script scans directories for new/modified files and updates the knowledge base
with extracted information, maintaining embeddings and relationships.
"""

import os
import sqlite3
import json
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import pandas as pd
import re
import statistics

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/processed/learning_log.json', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LearningManager:
    """è‡ªåŠ¨å­¦ä¹ ç®¡ç†å™¨"""

    def __init__(self, db_path: str = "knowledge_base.db"):
        self.db_path = db_path
        self.conn = None
        self.learning_stats = {
            'scan_time': datetime.now().isoformat(),
            'files_scanned': 0,
            'files_processed': 0,
            'entries_created': 0,
            'entries_updated': 0,
            'embeddings_regenerated': 0,
            'errors': []
        }

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA foreign_keys = ON")
            logger.info(f"Connected to database: {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")

    def get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºæ£€æµ‹å˜åŒ–"""
        try:
            # ä½¿ç”¨æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆç®€å•å“ˆå¸Œ
            stat = file_path.stat()
            hash_input = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
            return hashlib.md5(hash_input.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"Failed to get hash for {file_path}: {e}")
            return ""

    def get_processed_files(self) -> Dict[str, str]:
        """è·å–å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        try:
            # åˆ›å»ºå¤„ç†è®°å½•è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS processed_files (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT,
                    processed_at DATETIME,
                    entry_id INTEGER,
                    FOREIGN KEY (entry_id) REFERENCES knowledge_entries(id) ON DELETE SET NULL
                )
            """)

            cursor = self.conn.cursor()
            cursor.execute("SELECT file_path, file_hash FROM processed_files")
            return dict(cursor.fetchall())
        except Exception as e:
            logger.error(f"Failed to get processed files: {e}")
            return {}

    def scan_directories(self, directories: List[str], days_back: int = 7) -> List[Dict]:
        """æ‰«ææŒ‡å®šç›®å½•ä¸­çš„æ–°æ–‡ä»¶"""
        scanned_files = []
        cutoff_time = datetime.now() - timedelta(days=days_back)
        processed_files = self.get_processed_files()

        for directory in directories:
            if not os.path.exists(directory):
                logger.warning(f"Directory not found: {directory}")
                continue

            logger.info(f"Scanning directory: {directory}")

            for file_path in Path(directory).rglob('*'):
                if not file_path.is_file():
                    continue

                # æ£€æŸ¥æ–‡ä»¶ç±»å‹
                if file_path.suffix.lower() not in ['.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.csv']:
                    continue

                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                try:
                    mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if mod_time < cutoff_time:
                        continue
                except OSError:
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                rel_path = str(file_path)
                current_hash = self.get_file_hash(file_path)

                if rel_path in processed_files:
                    if processed_files[rel_path] == current_hash:
                        continue  # æ–‡ä»¶æœªå˜åŒ–ï¼Œè·³è¿‡

                # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
                file_info = {
                    'path': str(file_path),
                    'relative_path': rel_path,
                    'name': file_path.name,
                    'size': file_path.stat().st_size,
                    'modified': mod_time.isoformat(),
                    'hash': current_hash,
                    'type': file_path.suffix.lower()
                }

                scanned_files.append(file_info)

        self.learning_stats['files_scanned'] = len(scanned_files)
        logger.info(f"Found {len(scanned_files)} new/modified files")
        return scanned_files

    def determine_entity_type(self, file_path: str, content: str = "") -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œå†…å®¹ç¡®å®šå®ä½“ç±»å‹"""
        path_lower = file_path.lower()
        content_lower = content.lower() if content else ""

        # æŠ¥ä»·ç›¸å…³
        if any(keyword in path_lower or keyword in content_lower for keyword in
               ['æŠ¥ä»·', 'quote', 'ä»·æ ¼', 'price', 'è¯¢ä»·', 'inquiry']):
            if 'æŠ¥ä»·' in path_lower or 'quote' in path_lower or 'price' in path_lower:
                return 'quote'
            else:
                return 'inquiry'

        # å®¢æˆ·ç›¸å…³
        if any(keyword in path_lower for keyword in ['å®¢æˆ·', 'customer', 'å…¬å¸']):
            return 'customer'

        # å·¥å‚ç›¸å…³
        if any(keyword in path_lower for keyword in ['å·¥å‚', 'factory', 'ä¾›åº”å•†', 'supplier']):
            return 'factory'

        # å›¾çº¸ç›¸å…³
        if any(keyword in path_lower for keyword in ['å›¾çº¸', 'drawing', 'dwg']):
            return 'drawing'

        # äº§å“è§„æ ¼ç›¸å…³
        if any(keyword in path_lower for keyword in ['è§„æ ¼', 'specification', 'standard']):
            return 'specification'

        # ææ–™ç›¸å…³
        if any(keyword in path_lower for keyword in ['ææ–™', 'material']):
            return 'material'

        # é»˜è®¤ä¸ºäº§å“
        return 'product'

    def extract_text_from_file(self, file_path: str) -> Optional[str]:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            file_path = Path(file_path)

            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()

            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
                return df.to_string()

            elif file_path.suffix.lower() == '.csv':
                df = pd.read_csv(file_path, encoding='utf-8')
                return df.to_string()

            elif file_path.suffix.lower() == '.pdf':
                try:
                    import pdfplumber
                    text = ""
                    with pdfplumber.open(file_path) as pdf:
                        for page in pdf.pages:
                            text += page.extract_text() + "\n"
                    return text
                except ImportError:
                    logger.warning("pdfplumber not available, skipping PDF processing")
                    return None

            elif file_path.suffix.lower() in ['.docx', '.doc']:
                try:
                    import docx
                    doc = docx.Document(file_path)
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text
                except ImportError:
                    logger.warning("python-docx not available, skipping DOCX processing")
                    return None

            else:
                logger.warning(f"Unsupported file type: {file_path.suffix}")
                return None

        except Exception as e:
            logger.error(f"Failed to extract text from {file_path}: {e}")
            return None

    def extract_attributes_from_text(self, text: str, entity_type: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–å±æ€§ä¿¡æ¯"""
        attributes = {}

        try:
            # æå–äº§å“è§„æ ¼
            spec_patterns = [
                r'M\d+\.?\d*',     # Mè§„æ ¼
                r'Ï†\d+\.?\d*',     # ç›´å¾„
                r'\d+\.?\d*mm',    # æ¯«ç±³
                r'Ï†\d+\.?\d*',     # ç›´å¾„ç¬¦å·
                r'GB/T\s*\d+',     # å›½æ ‡
                r'ISO\s*\d+',      # ISOæ ‡å‡†
                r'DIN\s*\d+',      # DINæ ‡å‡†
            ]

            specs = []
            for pattern in spec_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                specs.extend(matches)

            if specs:
                attributes['specifications'] = list(set(specs))

            # æå–ææ–™ä¿¡æ¯
            materials = [
                'ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“œ', 'é“', 'é”Œ', 'å°¼é¾™', 'å¡‘æ–™', 'æ©¡èƒ¶',
                '304', '316', '201', '202', '45#', 'Q235', 'Q345', '20#', '40Cr'
            ]

            found_materials = []
            text_lower = text.lower()
            for material in materials:
                if material.lower() in text_lower:
                    found_materials.append(material)

            if found_materials:
                attributes['materials'] = list(set(found_materials))

            # æå–æ•°é‡ä¿¡æ¯
            quantity_patterns = [
                r'(\d+)\s*ä¸ª',
                r'(\d+)\s*ä»¶',
                r'(\d+)\s*å¥—',
                r'(\d+)\s*ç®±',
                r'(\d+)\s*åŒ…',
                r'quantity[:\s]+(\d+)',
                r'æ•°é‡[:\s]+(\d+)',
            ]

            quantities = []
            for pattern in quantity_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                quantities.extend(matches)

            if quantities:
                try:
                    attributes['quantity'] = int(quantities[0])
                except ValueError:
                    pass

            # æå–ä»·æ ¼ä¿¡æ¯ï¼ˆè„±æ•å¤„ç†ï¼‰
            price_patterns = [
                r'ï¿¥\s*([\d,]+\.?\d*)',
                r'Â¥\s*([\d,]+\.?\d*)',
                r'([\d,]+\.?\d*)\s*å…ƒ',
                r'RMB\s*([\d,]+\.?\d*)',
                r'([\d,]+\.?\d*)',
            ]

            prices = []
            for pattern in price_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    try:
                        price = float(match.replace(',', ''))
                        if 0.01 <= price <= 1000000:  # åˆç†çš„ä»·æ ¼èŒƒå›´
                            prices.append(price)
                    except ValueError:
                        continue

            if prices:
                # è®¡ç®—ä»·æ ¼ç»Ÿè®¡ï¼Œä½†ä¸å­˜å‚¨å…·ä½“ä»·æ ¼
                attributes['price_count'] = len(prices)
                attributes['price_range_min'] = min(prices)
                attributes['price_range_max'] = max(prices)
                if len(prices) > 1:
                    attributes['price_average'] = statistics.mean(prices)

            # æ ¹æ®å®ä½“ç±»å‹æå–ç‰¹å®šå±æ€§
            if entity_type == 'quote':
                # æå–æœ‰æ•ˆæœŸ
                validity_patterns = [
                    r'(\d+)\s*å¤©',
                    r'(\d+)\s*æ—¥',
                    r'æœ‰æ•ˆæœŸ[:\s]*(\d+)',
                ]
                for pattern in validity_patterns:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    if matches:
                        try:
                            attributes['validity_days'] = int(matches[0])
                            break
                        except ValueError:
                            continue

            elif entity_type == 'customer':
                # æå–è”ç³»æ–¹å¼ï¼ˆè„±æ•ï¼‰
                phone_patterns = [
                    r'1[3-9]\d{9}',  # ä¸­å›½æ‰‹æœºå·
                    r'\d{3}-\d{4}-\d{4}',  # åº§æœº
                ]

                phones = []
                for pattern in phone_patterns:
                    matches = re.findall(pattern, text)
                    phones.extend(matches)

                if phones:
                    attributes['has_phone'] = True
                    attributes['phone_count'] = len(phones)

                # æå–é‚®ç®±ï¼ˆè„±æ•ï¼‰
                email_patterns = [
                    r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
                ]

                emails = re.findall(email_patterns[0], text)
                if emails:
                    attributes['has_email'] = True
                    attributes['email_count'] = len(emails)

        except Exception as e:
            logger.error(f"Failed to extract attributes: {e}")

        return attributes

    def create_or_update_knowledge_entry(self, file_info: Dict, content: str, entity_type: str) -> Optional[int]:
        """åˆ›å»ºæˆ–æ›´æ–°çŸ¥è¯†æ¡ç›®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸å…³æ¡ç›®
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT id, name, description FROM knowledge_entries
                WHERE related_file = ?
            """, (file_info['relative_path'],))

            existing = cursor.fetchone()

            # æå–å±æ€§
            attributes = self.extract_attributes_from_text(content, entity_type)

            # ç”Ÿæˆåç§°å’Œæè¿°
            name = file_info['name']
            if len(name) > 200:
                name = name[:200]

            description = content[:1000] if content else "è‡ªåŠ¨æå–çš„æ–‡æ¡£å†…å®¹"

            if existing:
                # æ›´æ–°ç°æœ‰æ¡ç›®
                entry_id = existing[0]
                cursor.execute("""
                    UPDATE knowledge_entries
                    SET name = ?, description = ?, attributes_json = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (
                    name,
                    description,
                    json.dumps(attributes, ensure_ascii=False),
                    entry_id
                ))
                self.learning_stats['entries_updated'] += 1
                logger.info(f"Updated entry {entry_id}: {name}")
            else:
                # åˆ›å»ºæ–°æ¡ç›®
                cursor.execute("""
                    INSERT INTO knowledge_entries
                    (entity_type, name, description, related_file, attributes_json, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                """, (
                    entity_type,
                    name,
                    description,
                    file_info['relative_path'],
                    json.dumps(attributes, ensure_ascii=False)
                ))
                entry_id = cursor.lastrowid
                self.learning_stats['entries_created'] += 1
                logger.info(f"Created entry {entry_id}: {name}")

            # æ›´æ–°å¤„ç†è®°å½•
            cursor.execute("""
                INSERT OR REPLACE INTO processed_files
                (file_path, file_hash, processed_at, entry_id)
                VALUES (?, ?, CURRENT_TIMESTAMP, ?)
            """, (file_info['relative_path'], file_info['hash'], entry_id))

            self.conn.commit()
            return entry_id

        except Exception as e:
            logger.error(f"Failed to create/update knowledge entry: {e}")
            self.learning_stats['errors'].append(str(e))
            return None

    def learn_from_files(self, directories: List[str], days_back: int = 7) -> Dict:
        """ä»æ–‡ä»¶å­¦ä¹ """
        try:
            logger.info(f"ğŸš€ Starting learning from files (last {days_back} days)")

            # æ‰«ææ–‡ä»¶
            scanned_files = self.scan_directories(directories, days_back)

            if not scanned_files:
                logger.info("No new files to process")
                return self.learning_stats

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for file_info in scanned_files:
                try:
                    # æå–æ–‡æœ¬å†…å®¹
                    content = self.extract_text_from_file(file_info['path'])
                    if not content or len(content.strip()) < 50:
                        logger.warning(f"Insufficient content in {file_info['path']}, skipping")
                        continue

                    # ç¡®å®šå®ä½“ç±»å‹
                    entity_type = self.determine_entity_type(file_info['path'], content)

                    # åˆ›å»ºæˆ–æ›´æ–°çŸ¥è¯†æ¡ç›®
                    entry_id = self.create_or_update_knowledge_entry(file_info, content, entity_type)
                    if entry_id:
                        self.learning_stats['files_processed'] += 1

                except Exception as e:
                    logger.error(f"Failed to process file {file_info['path']}: {e}")
                    self.learning_stats['errors'].append(f"File {file_info['path']}: {str(e)}")
                    continue

            logger.info(f"âœ… Learning from files completed")
            logger.info(f"ğŸ“Š Processed: {self.learning_stats['files_processed']}/{self.learning_stats['files_scanned']} files")
            logger.info(f"ğŸ“ Created: {self.learning_stats['entries_created']} entries")
            logger.info(f"ğŸ”„ Updated: {self.learning_stats['entries_updated']} entries")

            return self.learning_stats

        except Exception as e:
            logger.error(f"âŒ Learning from files failed: {e}")
            self.learning_stats['errors'].append(str(e))
            return self.learning_stats

    def regenerate_embeddings_if_needed(self) -> bool:
        """å¦‚æœéœ€è¦ï¼Œé‡æ–°ç”ŸæˆåµŒå…¥ç´¢å¼•"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„çŸ¥è¯†æ¡ç›®éœ€è¦å»ºç«‹åµŒå…¥
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT COUNT(*) FROM knowledge_entries ke
                LEFT JOIN embedding_index ei ON ke.id = ei.entry_id
                WHERE ei.entry_id IS NULL
            """)
            missing_embeddings = cursor.fetchone()[0]

            if missing_embeddings > 0:
                logger.info(f"ğŸ”„ Regenerating embeddings for {missing_embeddings} new entries")

                # è°ƒç”¨åµŒå…¥æ„å»ºè„šæœ¬
                import subprocess
                result = subprocess.run([
                    'python3', 'build_embeddings.py', '--build'
                ], capture_output=True, text=True, cwd='.')

                if result.returncode == 0:
                    self.learning_stats['embeddings_regenerated'] = missing_embeddings
                    logger.info(f"âœ… Embeddings regenerated successfully")
                    return True
                else:
                    logger.error(f"âŒ Failed to regenerate embeddings: {result.stderr}")
                    return False
            else:
                logger.info("â„¹ï¸ All entries have embeddings, no regeneration needed")
                return True

        except Exception as e:
            logger.error(f"Failed to regenerate embeddings: {e}")
            return False

    def save_learning_stats(self) -> bool:
        """ä¿å­˜å­¦ä¹ ç»Ÿè®¡"""
        try:
            stats_file = Path("data/processed/learning_stats.json")
            stats_file.parent.mkdir(parents=True, exist_ok=True)

            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_stats, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“Š Learning stats saved to {stats_file}")
            return True

        except Exception as e:
            logger.error(f"Failed to save learning stats: {e}")
            return False

    def run_learning(self, mode: str = "weekly") -> Dict:
        """è¿è¡Œå­¦ä¹ è¿‡ç¨‹"""
        try:
            # è¿æ¥æ•°æ®åº“
            self.connect()

            # æ ¹æ®æ¨¡å¼é…ç½®æ‰«æå‚æ•°
            if mode == "weekly":
                days_back = 7
                directories = [
                    "/Users/gavin/Nutstore Files/.symlinks/åšæœäº‘/005-è¯¢ç›˜è¯¢ä»·å’Œ/",
                    "/Users/gavin/Nutstore Files/.symlinks/åšæœäº‘/002-å®¢æˆ·ä¸­/"
                ]
                auto_embed = True
            elif mode == "daily":
                days_back = 1
                directories = [
                    "/Users/gavin/Nutstore Files/.symlinks/åšæœäº‘/005-è¯¢ç›˜è¯¢ä»·å’Œ/",
                    "/Users/gavin/Nutstore Files/.symlinks/åšæœäº‘/002-å®¢æˆ·ä¸­/"
                ]
                auto_embed = False
            else:
                raise ValueError(f"Unknown mode: {mode}")

            # æ‰§è¡Œå­¦ä¹ 
            stats = self.learn_from_files(directories, days_back)

            # é‡æ–°ç”ŸæˆåµŒå…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if auto_embed:
                self.regenerate_embeddings_if_needed()

            # ä¿å­˜ç»Ÿè®¡
            self.save_learning_stats()

            return stats

        except Exception as e:
            logger.error(f"âŒ Learning process failed: {e}")
            self.learning_stats['errors'].append(str(e))
            return self.learning_stats
        finally:
            self.close()

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Learn From Updates')
    parser.add_argument('--mode', choices=['weekly', 'daily'], default='weekly',
                       help='Learning mode (weekly or daily)')
    parser.add_argument('--directories', nargs='+',
                       help='Specific directories to scan')
    parser.add_argument('--days-back', type=int, help='Number of days to look back')
    parser.add_argument('--force-embed', action='store_true',
                       help='Force embedding regeneration')

    args = parser.parse_args()

    learner = LearningManager()

    if args.directories:
        # è‡ªå®šä¹‰ç›®å½•æ‰«æ
        days = args.days_back or 7
        stats = learner.learn_from_files(args.directories, days)
    else:
        # æ ‡å‡†å­¦ä¹ æ¨¡å¼
        stats = learner.run_learning(args.mode)

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“ Learning Results ({args.mode} mode)")
    print("=" * 50)
    print(f"Files scanned: {stats['files_scanned']}")
    print(f"Files processed: {stats['files_processed']}")
    print(f"Entries created: {stats['entries_created']}")
    print(f"Entries updated: {stats['entries_updated']}")
    print(f"Embeddings regenerated: {stats['embeddings_regenerated']}")
    print(f"Errors: {len(stats['errors'])}")

    if stats['errors']:
        print(f"\nâŒ Errors:")
        for error in stats['errors'][:5]:  # Show first 5
            print(f"  - {error}")
        if len(stats['errors']) > 5:
            print(f"  ... and {len(stats['errors']) - 5} more")

if __name__ == "__main__":
    main()