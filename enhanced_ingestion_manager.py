#!/usr/bin/env python3
"""
å¢å¼ºå‹æ•°æ®å½•å…¥ç®¡ç†å™¨
æä¾›å®Œæ•´çš„æ—¥å¿—è®°å½•ã€é”™è¯¯å¤„ç†å’Œæ‰¹é‡è¿è¡Œèƒ½åŠ›
"""

import os
import logging
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import sqlite3

from models import DatabaseManager, Customer, Drawing

@dataclass
class IngestionResult:
    """å¯¼å…¥ç»“æœæ•°æ®ç±»"""
    success: bool
    message: str
    file_path: str = ""
    record_count: int = 0
    error_details: str = ""
    processing_time: float = 0.0

class EnhancedIngestionManager:
    """å¢å¼ºå‹æ•°æ®å½•å…¥ç®¡ç†å™¨"""

    def __init__(self, db_path: str = "./data/db.sqlite"):
        self.db_manager = DatabaseManager(db_path)
        self.setup_logging()
        self.setup_directories()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'successful_imports': 0,
            'failed_imports': 0,
            'total_records': 0,
            'start_time': None,
            'end_time': None
        }

    def setup_logging(self):
        """è®¾ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("./logs")
        log_dir.mkdir(exist_ok=True)

        # è®¾ç½®ä¸»æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'ingestion.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )

        # è®¾ç½®é”™è¯¯æ—¥å¿—
        error_handler = logging.FileHandler(log_dir / 'ingestion_errors.log', encoding='utf-8')
        error_handler.setLevel(logging.ERROR)

        self.logger = logging.getLogger('EnhancedIngestion')
        self.logger.addHandler(error_handler)

        # è®¾ç½®è¯¦ç»†å¤„ç†æ—¥å¿—
        self.process_logger = logging.getLogger('Processing')
        process_handler = logging.FileHandler(log_dir / 'processing_details.log', encoding='utf-8')
        process_handler.setLevel(logging.DEBUG)
        process_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        self.process_logger.addHandler(process_handler)

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        dirs = [
            './data/processed',
            './data/failed',
            './data/backups',
            './logs',
            './reports'
        ]

        for dir_path in dirs:
            Path(dir_path).mkdir(exist_ok=True)

    def log_ingestion_start(self, operation: str, source_path: str):
        """è®°å½•å¯¼å…¥å¼€å§‹"""
        self.logger.info(f"ğŸš€ å¼€å§‹{operation}: {source_path}")
        self.process_logger.info(f"START: {operation} | {source_path} | {datetime.now().isoformat()}")

    def log_ingestion_end(self, operation: str, result: IngestionResult):
        """è®°å½•å¯¼å…¥ç»“æŸ"""
        status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±è´¥"
        self.logger.info(f"{status} {operation}: {result.message}")

        if result.success:
            self.process_logger.info(f"SUCCESS: {operation} | æ–‡ä»¶: {result.file_path} | "
                                   f"è®°å½•æ•°: {result.record_count} | è€—æ—¶: {result.processing_time:.2f}s")
        else:
            self.process_logger.error(f"FAILED: {operation} | æ–‡ä»¶: {result.file_path} | "
                                    f"é”™è¯¯: {result.error_details} | è€—æ—¶: {result.processing_time:.2f}s")

    def safe_execute_with_retry(self, operation_func, operation_name: str, max_retries: int = 3) -> IngestionResult:
        """å¸¦é‡è¯•æœºåˆ¶çš„å®‰å…¨æ‰§è¡Œ"""
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                result = operation_func()
                processing_time = time.time() - start_time

                if isinstance(result, IngestionResult):
                    result.processing_time = processing_time
                else:
                    result = IngestionResult(
                        success=True,
                        message=f"{operation_name}å®Œæˆ",
                        processing_time=processing_time
                    )

                if attempt > 0:
                    self.logger.info(f"ğŸ”„ é‡è¯•æˆåŠŸ: {operation_name} (ç¬¬{attempt + 1}æ¬¡å°è¯•)")

                return result

            except Exception as e:
                error_msg = f"{operation_name}å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡å°è¯•): {str(e)}"
                self.logger.error(error_msg)

                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›é”™è¯¯ç»“æœ
                    return IngestionResult(
                        success=False,
                        message=f"{operation_name}å¤±è´¥",
                        error_details=error_msg,
                        processing_time=0.0
                    )
                else:
                    # ç­‰å¾…åé‡è¯•
                    wait_time = (attempt + 1) * 2
                    self.logger.info(f"â³ ç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)

    def ingest_customers_batch(self, source_paths: List[str]) -> List[IngestionResult]:
        """æ‰¹é‡å¯¼å…¥å®¢æˆ·æ•°æ®"""
        self.logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¯¼å…¥å®¢æˆ·æ•°æ®: {len(source_paths)} ä¸ªæ–‡ä»¶")
        self.stats['start_time'] = datetime.now()

        results = []
        total_records = 0

        for source_path in source_paths:
            self.log_ingestion_start("å®¢æˆ·å¯¼å…¥", source_path)

            result = self.safe_execute_with_retry(
                lambda: self._ingest_single_customer_file(source_path),
                f"å®¢æˆ·æ–‡ä»¶å¯¼å…¥: {Path(source_path).name}",
                max_retries=2
            )

            results.append(result)
            self.log_ingestion_end("å®¢æˆ·å¯¼å…¥", result)

            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_processed'] += 1
            if result.success:
                self.stats['successful_imports'] += 1
                total_records += result.record_count
            else:
                self.stats['failed_imports'] += 1

                # ç§»åŠ¨å¤±è´¥æ–‡ä»¶åˆ°å¤±è´¥ç›®å½•
                self._move_failed_file(source_path, "customer")

        self.stats['total_records'] = total_records
        self.stats['end_time'] = datetime.now()

        # ç”Ÿæˆæ‰¹é‡æŠ¥å‘Š
        self._generate_batch_report("customer_import", results)

        return results

    def ingest_drawings_batch(self, source_paths: List[str]) -> List[IngestionResult]:
        """æ‰¹é‡å¯¼å…¥å›¾çº¸æ•°æ®"""
        self.logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¯¼å…¥å›¾çº¸æ•°æ®: {len(source_paths)} ä¸ªæ–‡ä»¶")
        self.stats['start_time'] = datetime.now()

        results = []
        total_records = 0

        for source_path in source_paths:
            self.log_ingestion_start("å›¾çº¸å¯¼å…¥", source_path)

            result = self.safe_execute_with_retry(
                lambda: self._ingest_single_drawing_file(source_path),
                f"å›¾çº¸æ–‡ä»¶å¯¼å…¥: {Path(source_path).name}",
                max_retries=2
            )

            results.append(result)
            self.log_ingestion_end("å›¾çº¸å¯¼å…¥", result)

            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_processed'] += 1
            if result.success:
                self.stats['successful_imports'] += 1
                total_records += result.record_count
            else:
                self.stats['failed_imports'] += 1

                # ç§»åŠ¨å¤±è´¥æ–‡ä»¶åˆ°å¤±è´¥ç›®å½•
                self._move_failed_file(source_path, "drawing")

        self.stats['total_records'] = total_records
        self.stats['end_time'] = datetime.now()

        # ç”Ÿæˆæ‰¹é‡æŠ¥å‘Š
        self._generate_batch_report("drawing_import", results)

        return results

    def _ingest_single_customer_file(self, file_path: str) -> IngestionResult:
        """å¯¼å…¥å•ä¸ªå®¢æˆ·æ–‡ä»¶"""
        try:
            from ingest_customers import CustomerIngestor
            ingestor = CustomerIngestor(self.db_manager.db_path)

            # è¯»å–å’Œå¤„ç†æ–‡ä»¶
            customers = ingestor.process_file(file_path)

            if not customers:
                return IngestionResult(
                    success=True,
                    message="æ²¡æœ‰æ‰¾åˆ°å®¢æˆ·æ•°æ®",
                    file_path=file_path,
                    record_count=0
                )

            # æ’å…¥æ•°æ®åº“
            inserted_count = 0
            for customer_data in customers:
                try:
                    ingestor.customer.create(**customer_data)
                    inserted_count += 1
                except Exception as e:
                    self.logger.warning(f"æ’å…¥å®¢æˆ·å¤±è´¥: {customer_data.get('company_name', 'Unknown')} - {e}")

            return IngestionResult(
                success=True,
                message=f"æˆåŠŸå¯¼å…¥ {inserted_count} ä¸ªå®¢æˆ·",
                file_path=file_path,
                record_count=inserted_count
            )

        except Exception as e:
            return IngestionResult(
                success=False,
                message=f"å®¢æˆ·æ–‡ä»¶å¤„ç†å¤±è´¥",
                file_path=file_path,
                error_details=f"{str(e)}\n{traceback.format_exc()}"
            )

    def _ingest_single_drawing_file(self, file_path: str) -> IngestionResult:
        """å¯¼å…¥å•ä¸ªå›¾çº¸æ–‡ä»¶"""
        try:
            from ingest_drawings import DrawingIngestor
            ingestor = DrawingIngestor(self.db_manager.db_path)

            # å¤„ç†æ–‡ä»¶
            drawing_data = ingestor.process_drawing_file(file_path)

            if not drawing_data:
                return IngestionResult(
                    success=True,
                    message="æ²¡æœ‰æå–åˆ°å›¾çº¸æ•°æ®",
                    file_path=file_path,
                    record_count=0
                )

            # æ’å…¥æ•°æ®åº“
            drawing_id = ingestor.drawing.create(**drawing_data)

            return IngestionResult(
                success=True,
                message=f"æˆåŠŸå¯¼å…¥å›¾çº¸: {drawing_data.get('drawing_name', 'Unknown')}",
                file_path=file_path,
                record_count=1
            )

        except Exception as e:
            return IngestionResult(
                success=False,
                message=f"å›¾çº¸æ–‡ä»¶å¤„ç†å¤±è´¥",
                file_path=file_path,
                error_details=f"{str(e)}\n{traceback.format_exc()}"
            )

    def _move_failed_file(self, file_path: str, file_type: str):
        """ç§»åŠ¨å¤±è´¥çš„æ–‡ä»¶åˆ°å¤±è´¥ç›®å½•"""
        try:
            source = Path(file_path)
            if source.exists():
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{timestamp}_{file_type}_{source.name}"
                destination = Path("./data/failed") / filename

                # å¦‚æœæ˜¯ç§»åŠ¨æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åˆ é™¤
                if source.is_file():
                    import shutil
                    shutil.move(str(source), str(destination))
                    self.logger.info(f"ğŸ“ å¤±è´¥æ–‡ä»¶å·²ç§»åŠ¨: {destination}")
        except Exception as e:
            self.logger.error(f"ç§»åŠ¨å¤±è´¥æ–‡ä»¶å‡ºé”™: {e}")

    def _generate_batch_report(self, operation: str, results: List[IngestionResult]):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        report = {
            'operation': operation,
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'results': [asdict(r) for r in results],
            'summary': {
                'total_files': len(results),
                'successful': len([r for r in results if r.success]),
                'failed': len([r for r in results if not r.success]),
                'total_records': sum(r.record_count for r in results if r.success),
                'total_processing_time': sum(r.processing_time for r in results)
            }
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"./reports/{operation}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # ä¿å­˜æœ€æ–°æŠ¥å‘Š
        latest_report = f"./reports/latest_{operation}_report.json"
        with open(latest_report, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

    def create_database_backup(self, operation: str):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"./data/backups/db_backup_{operation}_{timestamp}.sqlite"

            # ç®€å•çš„æ•°æ®åº“å¤åˆ¶
            import shutil
            shutil.copy2(self.db_manager.db_path, backup_file)

            self.logger.info(f"ğŸ’¾ æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
            return backup_file

        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return None

    def run_scheduled_import(self, customer_paths: List[str], drawing_paths: List[str]):
        """è¿è¡Œè®¡åˆ’å¯¼å…¥"""
        self.logger.info("ğŸ”„ å¼€å§‹è®¡åˆ’å¯¼å…¥æµç¨‹")

        # åˆ›å»ºå¤‡ä»½
        backup_file = self.create_database_backup("scheduled_import")

        try:
            # å¯¼å…¥å®¢æˆ·æ•°æ®
            if customer_paths:
                customer_results = self.ingest_customers_batch(customer_paths)
                self.logger.info(f"å®¢æˆ·å¯¼å…¥å®Œæˆ: æˆåŠŸ {len([r for r in customer_results if r.success])}/{len(customer_results)}")

            # å¯¼å…¥å›¾çº¸æ•°æ®
            if drawing_paths:
                drawing_results = self.ingest_drawings_batch(drawing_paths)
                self.logger.info(f"å›¾çº¸å¯¼å…¥å®Œæˆ: æˆåŠŸ {len([r for r in drawing_results if r.success])}/{len(drawing_results)}")

            self.logger.info("âœ… è®¡åˆ’å¯¼å…¥å®Œæˆ")
            return True

        except Exception as e:
            self.logger.error(f"è®¡åˆ’å¯¼å…¥å¤±è´¥: {e}")

            # å¦‚æœæœ‰å¤‡ä»½ï¼Œè¯¢é—®æ˜¯å¦æ¢å¤
            if backup_file:
                self.logger.info(f"ğŸ’¡ å¤‡ä»½æ–‡ä»¶å¯ç”¨: {backup_file}")

            return False

    def get_import_statistics(self) -> Dict[str, Any]:
        """è·å–å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_session': self.stats,
            'database_stats': self._get_database_stats(),
            'recent_logs': self._get_recent_logs()
        }

    def _get_database_stats(self) -> Dict[str, int]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡"""
        try:
            with self.db_manager:
                conn = self.db_manager.connect()
                cursor = conn.cursor()

                stats = {}
                tables = ['customers', 'drawings', 'factories', 'factory_quotes', 'specifications']

                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    stats[table] = cursor.fetchone()[0]

                return stats

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def _get_recent_logs(self) -> List[str]:
        """è·å–æœ€è¿‘çš„æ—¥å¿—"""
        try:
            log_file = Path("./logs/ingestion.log")
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    return [line.strip() for line in lines[-10:]]  # æœ€è¿‘10è¡Œ
            return []
        except Exception:
            return []

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    manager = EnhancedIngestionManager()

    # ç¤ºä¾‹ï¼šæ‰¹é‡å¯¼å…¥
    customer_files = [
        # "/path/to/customer_excel.xlsx",
        # "/path/to/customer_data.csv"
    ]

    drawing_files = [
        # "/path/to/drawings_folder"
    ]

    # è¿è¡Œè®¡åˆ’å¯¼å…¥
    success = manager.run_scheduled_import(customer_files, drawing_files)

    # æ˜¾ç¤ºç»Ÿè®¡
    stats = manager.get_import_statistics()
    print(f"å¯¼å…¥ç»Ÿè®¡: {json.dumps(stats, ensure_ascii=False, indent=2)}")

if __name__ == "__main__":
    main()