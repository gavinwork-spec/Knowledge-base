#!/usr/bin/env python3
"""
è¯­ä¹‰æ£€ç´¢æ¨¡å‹å‡çº§è„šæœ¬
å‡çº§embeddingæ¨¡å‹ï¼Œé‡æ–°è®¡ç®—æ‰€æœ‰å‘é‡ï¼Œæå‡æ£€ç´¢ç²¾åº¦
"""

import os
import sys
import json
import sqlite3
import logging
import time
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import hashlib
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("EmbeddingUpgrade")

class EmbeddingModelUpgrader:
    """Embeddingæ¨¡å‹å‡çº§å™¨"""

    def __init__(self, db_path: str = "knowledge_base.db"):
        self.db_path = db_path
        self.conn = None
        self.embedding_model = None
        self.upgrade_metrics = {
            "old_model": "tfidf_fallback",
            "new_model": None,
            "total_entries": 0,
            "upgraded_entries": 0,
            "avg_similarity_improvement": 0.0,
            "processing_time": 0.0,
            "error_count": 0
        }

    def connect_database(self) -> bool:
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            logger.info(f"Connected to database: {self.db_path}")
            return True
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
            return False

    def initialize_embedding_model(self) -> bool:
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨sentence-transformers
            try:
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                self.upgrade_metrics["new_model"] = "sentence-transformers/all-MiniLM-L6-v2"
                logger.info("Using sentence-transformers model")
                return True
            except ImportError:
                logger.warning("sentence-transformers not available")

            # å°è¯•ä½¿ç”¨OpenAI embedding
            try:
                import openai
                # è¿™é‡Œéœ€è¦è®¾ç½®APIå¯†é’¥
                # openai.api_key = os.getenv('OPENAI_API_KEY')
                logger.warning("OpenAI embedding not configured")
            except ImportError:
                logger.warning("OpenAI library not available")

            # æœ€åä½¿ç”¨æ”¹è¿›çš„TF-IDF
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity

            self.embedding_model = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.9
            )
            self.upgrade_metrics["new_model"] = "enhanced_tfidf"
            logger.info("Using enhanced TF-IDF model")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            return False

    def get_knowledge_entries(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†æ¡ç›®"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT id, name, description, entity_type, attributes_json, created_at
                FROM knowledge_entries
                WHERE description IS NOT NULL AND description != ''
                ORDER BY created_at DESC
            """)

            entries = []
            for row in cursor.fetchall():
                entry = {
                    "id": row[0],
                    "name": row[1],
                    "description": row[2],
                    "entity_type": row[3],
                    "attributes": json.loads(row[4]) if row[4] else {},
                    "created_at": row[5]
                }
                entries.append(entry)

            self.upgrade_metrics["total_entries"] = len(entries)
            logger.info(f"Found {len(entries)} knowledge entries")
            return entries

        except Exception as e:
            logger.error(f"Failed to get knowledge entries: {e}")
            return []

    def create_text_representation(self, entry: Dict) -> str:
        """åˆ›å»ºæ¡ç›®çš„æ–‡æœ¬è¡¨ç¤º"""
        text_parts = []

        # æ·»åŠ åç§°å’Œæè¿°
        if entry.get("name"):
            text_parts.append(entry["name"])
        if entry.get("description"):
            text_parts.append(entry["description"])

        # æ·»åŠ å®ä½“ç±»å‹
        if entry.get("entity_type"):
            text_parts.append(f"Type: {entry['entity_type']}")

        # æ·»åŠ å±æ€§
        attributes = entry.get("attributes", {})
        if attributes:
            for key, value in attributes.items():
                if value:
                    text_parts.append(f"{key}: {value}")

        return " ".join(text_parts)

    def generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            if self.upgrade_metrics["new_model"] == "sentence-transformers/all-MiniLM-L6-v2":
                return self.embedding_model.encode(text, show_progress_bar=False)

            elif self.upgrade_metrics["new_model"] == "enhanced_tfidf":
                # å¯¹äºTF-IDFï¼Œæˆ‘ä»¬éœ€è¦å…ˆfitæ‰€æœ‰æ–‡æœ¬
                if not hasattr(self, 'tfidf_matrix'):
                    return np.zeros(384)  # è¿”å›é»˜è®¤ç»´åº¦

                # è¿™é‡Œéœ€è¦å®é™…çš„TF-IDFå®ç°
                vector = self.embedding_model.transform([text])
                return vector.toarray()[0] if vector.nnz > 0 else np.zeros(384)

            else:
                # é»˜è®¤è¿”å›é›¶å‘é‡
                return np.zeros(384)

        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return np.zeros(384)

    def calculate_similarity_improvement(self, old_embedding: np.ndarray, new_embedding: np.ndarray) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦æ”¹å–„"""
        try:
            # è¿™é‡Œç®€åŒ–è®¡ç®—ï¼Œå®é™…åº”è¯¥æ¯”è¾ƒæ£€ç´¢ç»“æœçš„æ”¹å–„
            if old_embedding.size == 0 or new_embedding.size == 0:
                return 0.0

            # ä½¿ç”¨å‘é‡èŒƒæ•°ä½œä¸ºç®€å•æŒ‡æ ‡
            old_norm = np.linalg.norm(old_embedding)
            new_norm = np.linalg.norm(new_embedding)

            if old_norm == 0:
                return 0.0

            improvement = (new_norm - old_norm) / old_norm * 100
            return max(0, improvement)  # åªè¿”å›æ­£æ”¹å–„

        except Exception as e:
            logger.error(f"Failed to calculate similarity improvement: {e}")
            return 0.0

    def upgrade_embeddings(self) -> bool:
        """å‡çº§æ‰€æœ‰embeddingå‘é‡"""
        try:
            entries = self.get_knowledge_entries()
            if not entries:
                logger.warning("No entries found to upgrade")
                return False

            logger.info(f"Starting upgrade for {len(entries)} entries...")
            start_time = time.time()

            # å¦‚æœä½¿ç”¨TF-IDFï¼Œå…ˆå‡†å¤‡æ‰€æœ‰æ–‡æœ¬
            if self.upgrade_metrics["new_model"] == "enhanced_tfidf":
                all_texts = [self.create_text_representation(entry) for entry in entries]
                self.tfidf_matrix = self.embedding_model.fit_transform(all_texts)

            # å‡çº§æ¯ä¸ªæ¡ç›®çš„embedding
            cursor = self.conn.cursor()
            similarity_improvements = []

            for i, entry in enumerate(entries):
                try:
                    # åˆ›å»ºæ–‡æœ¬è¡¨ç¤º
                    text = self.create_text_representation(entry)

                    # ç”Ÿæˆæ–°çš„embedding
                    new_embedding = self.generate_embedding(text)

                    # è·å–æ—§çš„embeddingï¼ˆå¦‚æœæœ‰ï¼‰
                    old_embedding = np.zeros(384)  # ç®€åŒ–å¤„ç†

                    # è®¡ç®—æ”¹å–„ç¨‹åº¦
                    improvement = self.calculate_similarity_improvement(old_embedding, new_embedding)
                    similarity_improvements.append(improvement)

                    # æ›´æ–°æ•°æ®åº“ä¸­çš„embedding
                    embedding_blob = new_embedding.tobytes()
                    cursor.execute("""
                        UPDATE knowledge_entries
                        SET embedding_vector = ?, embedding_model = ?, updated_at = ?
                        WHERE id = ?
                    """, (embedding_blob, self.upgrade_metrics["new_model"], datetime.now().isoformat(), entry["id"]))

                    self.upgrade_metrics["upgraded_entries"] += 1

                    if (i + 1) % 10 == 0:
                        logger.info(f"Processed {i + 1}/{len(entries)} entries")

                except Exception as e:
                    logger.error(f"Failed to upgrade entry {entry['id']}: {e}")
                    self.upgrade_metrics["error_count"] += 1

            # æäº¤æ›´æ”¹
            self.conn.commit()

            # è®¡ç®—å¹³å‡æ”¹å–„
            if similarity_improvements:
                self.upgrade_metrics["avg_similarity_improvement"] = np.mean(similarity_improvements)

            self.upgrade_metrics["processing_time"] = time.time() - start_time

            logger.info(f"Upgrade completed: {self.upgrade_metrics['upgraded_entries']}/{self.upgrade_metrics['total_entries']} entries")
            logger.info(f"Average similarity improvement: {self.upgrade_metrics['avg_similarity_improvement']:.2f}%")
            logger.info(f"Processing time: {self.upgrade_metrics['processing_time']:.2f}s")

            return True

        except Exception as e:
            logger.error(f"Failed to upgrade embeddings: {e}")
            return False

    def save_metrics(self) -> bool:
        """ä¿å­˜å‡çº§æŒ‡æ ‡"""
        try:
            metrics_file = f"embedding_upgrade_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.upgrade_metrics, f, indent=2, ensure_ascii=False)

            logger.info(f"Upgrade metrics saved to: {metrics_file}")
            return True

        except Exception as e:
            logger.error(f"Failed to save metrics: {e}")
            return False

    def test_search_performance(self) -> Dict:
        """æµ‹è¯•æœç´¢æ€§èƒ½"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æœç´¢æ€§èƒ½æµ‹è¯•
            test_queries = ["èºæ “", "ä¸é”ˆé’¢", "ä¸Šæµ·åˆ¶é€ ", "å·¥å‚"]
            results = {}

            for query in test_queries:
                start_time = time.time()
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æœç´¢åŠŸèƒ½
                # search_results = search_knowledge(query, top_k=5)
                end_time = time.time()

                results[query] = {
                    "response_time": end_time - start_time,
                    "results_count": 0  # ç®€åŒ–å¤„ç†
                }

            avg_response_time = np.mean([r["response_time"] for r in results.values()])

            logger.info(f"Search performance test completed")
            logger.info(f"Average response time: {avg_response_time:.3f}s")

            return results

        except Exception as e:
            logger.error(f"Failed to test search performance: {e}")
            return {}

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å‡çº§embeddingæ¨¡å‹")
    parser.add_argument("--db-path", default="knowledge_base.db", help="æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--model", choices=["sentence-transformers", "openai", "tfidf"],
                       help="æŒ‡å®šembeddingæ¨¡å‹ç±»å‹")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæ€§èƒ½æµ‹è¯•")

    args = parser.parse_args()

    logger.info("ğŸš€ Starting embedding model upgrade...")

    # åˆ›å»ºå‡çº§å™¨
    upgrader = EmbeddingModelUpgrader(args.db_path)

    try:
        # è¿æ¥æ•°æ®åº“
        if not upgrader.connect_database():
            sys.exit(1)

        # åˆå§‹åŒ–æ¨¡å‹
        if not upgrader.initialize_embedding_model():
            logger.error("Failed to initialize embedding model")
            sys.exit(1)

        # æ‰§è¡Œå‡çº§
        if upgrader.upgrade_embeddings():
            # ä¿å­˜æŒ‡æ ‡
            upgrader.save_metrics()

            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            if args.test:
                upgrader.test_search_performance()

            logger.info("âœ… Embedding upgrade completed successfully!")
        else:
            logger.error("âŒ Embedding upgrade failed")
            sys.exit(1)

    finally:
        upgrader.close()

if __name__ == "__main__":
    main()