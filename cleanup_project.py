#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Project Cleanup Script
é¡¹ç›®æ¸…ç†è„šæœ¬

Clean up unnecessary files, organize the project structure, and prepare for GitHub upload.
"""

import os
import shutil
import glob
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProjectCleanup:
    """é¡¹ç›®æ¸…ç†å™¨"""

    def __init__(self, project_path="/Users/gavin/Knowledge base"):
        self.project_path = project_path
        self.cleanup_log = []
        self.backup_created = False

    def run_cleanup(self):
        """æ‰§è¡Œå®Œæ•´çš„é¡¹ç›®æ¸…ç†"""
        logger.info("ğŸ§¹ Starting project cleanup...")

        # 1. åˆ›å»ºå¤‡ä»½
        self.create_backup()

        # 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜
        self.cleanup_temp_files()

        # 3. æ¸…ç†é‡å¤æ–‡ä»¶
        self.cleanup_duplicate_files()

        # 4. æ•´ç†é¡¹ç›®ç»“æ„
        self.organize_project_structure()

        # 5. æ¸…ç†æ—¥å¿—æ–‡ä»¶
        self.cleanup_log_files()

        # 6. æ¸…ç†JSONæŠ¥å‘Šæ–‡ä»¶
        self.cleanup_json_reports()

        # 7. æ•´ç†é…ç½®æ–‡ä»¶
        self.organize_config_files()

        # 8. åˆ›å»ºæ›´æ–°çš„README
        self.create_updated_readme()

        # 9. ç”Ÿæˆé¡¹ç›®ç»Ÿè®¡
        self.generate_project_stats()

        # 10. åˆ›å»ºéƒ¨ç½²æ¸…å•
        self.create_deployment_checklist()

        self.save_cleanup_log()
        logger.info("âœ… Project cleanup completed successfully!")

    def create_backup(self):
        """åˆ›å»ºé¡¹ç›®å¤‡ä»½"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = f"backup_{timestamp}"

            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_path = os.path.join(self.project_path, backup_dir)
            os.makedirs(backup_path, exist_ok=True)

            # å¤‡ä»½é‡è¦é…ç½®æ–‡ä»¶
            important_files = [
                ".env",
                ".gitignore",
                "requirements.txt",
                "docker-compose.yml",
                "openapi_spec.yaml"
            ]

            for file in important_files:
                src = os.path.join(self.project_path, file)
                if os.path.exists(src):
                    dst = os.path.join(backup_path, file)
                    shutil.copy2(src, dst)
                    logger.info(f"  Backed up: {file}")

            self.backup_created = True
            logger.info(f"âœ… Created backup in {backup_dir}")

        except Exception as e:
            logger.error(f"Failed to create backup: {e}")

    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        temp_patterns = [
            "*.tmp",
            "*.temp",
            "*.cache",
            "__pycache__",
            "*.pyc",
            "*.pyo",
            ".DS_Store",
            "*.swp",
            "*.swo",
            "*~"
        ]

        cleaned_files = []
        for pattern in temp_patterns:
            for file_path in glob.glob(os.path.join(self.project_path, "**", pattern), recursive=True):
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        cleaned_files.append(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                        cleaned_files.append(file_path)
                except Exception as e:
                    logger.warning(f"Could not remove {file_path}: {e}")

        if cleaned_files:
            logger.info(f"ğŸ—‘ï¸ Removed {len(cleaned_files)} temporary files")
            self.cleanup_log.append(f"Temporary files removed: {len(cleaned_files)}")

    def cleanup_duplicate_files(self):
        """æ¸…ç†é‡å¤æ–‡ä»¶"""
        duplicate_patterns = [
            "*_old*",
            "*_backup*",
            "*_copy*",
            "*_duplicate*"
        ]

        # ä¿ç•™çš„é‡è¦æ–‡ä»¶æ¨¡å¼
        keep_patterns = [
            "README*",
            "*.md",
            "requirements*.txt",
            "setup.py",
            "main.py"
        ]

        removed_count = 0
        for pattern in duplicate_patterns:
            for file_path in glob.glob(os.path.join(self.project_path, "**", pattern), recursive=True):
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡è¦æ–‡ä»¶
                should_keep = any(file_path.endswith(keep_pat) for keep_pat in keep_patterns)

                if not should_keep and os.path.exists(file_path):
                    try:
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                            removed_count += 1
                        elif os.path.isdir(file_path):
                            shutil.rmtree(file_path)
                            removed_count += 1
                    except Exception as e:
                        logger.warning(f"Could not remove {file_path}: {e}")

        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ Removed {removed_count} duplicate files")
            self.cleanup_log.append(f"Duplicate files removed: {removed_count}")

    def organize_project_structure(self):
        """æ•´ç†é¡¹ç›®ç»“æ„"""
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        required_dirs = [
            "docs",
            "tests",
            "scripts",
            "config",
            "data",
            "logs"
        ]

        for dir_name in required_dirs:
            dir_path = os.path.join(self.project_path, dir_name)
            os.makedirs(dir_path, exist_ok=True)

        # ç§»åŠ¨æ–‡æ¡£æ–‡ä»¶åˆ°docsç›®å½•
        doc_patterns = ["*.md", "DOCUMENTATION*"]
        for pattern in doc_patterns:
            for file_path in glob.glob(os.path.join(self.project_path, pattern)):
                if os.path.isfile(file_path) and not file_path.endswith("README.md"):
                    try:
                        filename = os.path.basename(file_path)
                        dst = os.path.join(self.project_path, "docs", filename)
                        shutil.move(file_path, dst)
                        logger.info(f"  Moved to docs/: {filename}")
                    except Exception as e:
                        logger.warning(f"Could not move {file_path}: {e}")

        # ç§»åŠ¨æµ‹è¯•æ–‡ä»¶åˆ°testsç›®å½•
        test_patterns = ["test_*.py", "*_test.py", "verify_*.py"]
        for pattern in test_patterns:
            for file_path in glob.glob(os.path.join(self.project_path, pattern)):
                try:
                    filename = os.path.basename(file_path)
                    dst = os.path.join(self.project_path, "tests", filename)
                    shutil.move(file_path, dst)
                    logger.info(f"  Moved to tests/: {filename}")
                except Exception as e:
                    logger.warning(f"Could not move {file_path}: {e}")

    def cleanup_log_files(self):
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        log_files = glob.glob(os.path.join(self.project_path, "**", "*.log"), recursive=True)

        # åªä¿ç•™æœ€è¿‘çš„æ—¥å¿—æ–‡ä»¶
        log_files.sort(key=os.path.getmtime, reverse=True)
        recent_logs = log_files[:5]  # ä¿ç•™æœ€è¿‘5ä¸ªæ—¥å¿—æ–‡ä»¶

        for log_file in log_files[5:]:
            try:
                os.remove(log_file)
                logger.info(f"  Removed old log: {os.path.basename(log_file)}")
            except Exception as e:
                logger.warning(f"Could not remove {log_file}: {e}")

    def cleanup_json_reports(self):
        """æ¸…ç†JSONæŠ¥å‘Šæ–‡ä»¶"""
        json_files = glob.glob(os.path.join(self.project_path, "**", "*.json"), recursive=True)

        # ç§»åŠ¨æŠ¥å‘Šæ–‡ä»¶åˆ°ä¸“é—¨çš„ç›®å½•
        reports_dir = os.path.join(self.project_path, "data", "reports")
        os.makedirs(reports_dir, exist_ok=True)

        moved_count = 0
        for json_file in json_files:
            if "verification_report" in json_file or "metrics_report" in json_file:
                try:
                    filename = os.path.basename(json_file)
                    dst = os.path.join(reports_dir, filename)
                    shutil.move(json_file, dst)
                    moved_count += 1
                    logger.info(f"  Moved to reports/: {filename}")
                except Exception as e:
                    logger.warning(f"Could not move {json_file}: {e}")

        if moved_count > 0:
            logger.info(f"ğŸ“Š Moved {moved_count} JSON report files to data/reports/")

    def organize_config_files(self):
        """æ•´ç†é…ç½®æ–‡ä»¶"""
        config_dir = os.path.join(self.project_path, "config")
        os.makedirs(config_dir, exist_ok=True)

        # ç§»åŠ¨YAMLé…ç½®æ–‡ä»¶
        yaml_files = glob.glob(os.path.join(self.project_path, "*.yaml"))
        for yaml_file in yaml_files:
            try:
                filename = os.path.basename(yaml_file)
                dst = os.path.join(config_dir, filename)
                shutil.move(yaml_file, dst)
                logger.info(f"  Moved to config/: {filename}")
            except Exception as e:
                logger.warning(f"Could not move {yaml_file}: {e}")

    def create_updated_readme(self):
        """åˆ›å»ºæ›´æ–°çš„README"""
        readme_content = """# Manufacturing Knowledge Base System

A comprehensive AI-powered knowledge base system specifically designed for manufacturing operations, featuring advanced RAG capabilities, multi-agent orchestration, and real-time observability.

## ğŸš€ Key Features

### ğŸ¤– Advanced AI Capabilities
- **Advanced RAG System**: State-of-the-art retrieval with LangChain and LlamaIndex integration
- **Multi-Agent Orchestration**: Intelligent agent coordination for complex tasks
- **Multi-Modal Processing**: Handle text, images, tables, and technical drawings
- **Query Decomposition**: Break down complex manufacturing queries
- **Conversation Memory**: Context-aware dialogue management

### ğŸ­ Manufacturing-Specific Features
- **Quote Management**: Automated quote generation and analysis
- **Quality Control**: Integrated quality assurance workflows
- **Compliance Tracking**: ISO and industry standard compliance
- **Document Processing**: Technical drawing and specification analysis
- **Safety Management**: Safety procedure enforcement and monitoring

### ğŸ“Š Comprehensive Observability
- **Real-time Monitoring**: WebSocket-based dashboard with live metrics
- **AI Interaction Tracking**: Detailed logging with LangFuse patterns
- **Cost Analysis**: API call cost breakdown and forecasting
- **User Analytics**: Behavior pattern recognition and insights
- **Intelligent Alerting**: Proactive anomaly detection and notification

### ğŸ” Advanced Search & Retrieval
- **Hybrid Search Engine**: Multiple search strategies combined
- **Personalized Search**: User-adaptive search results
- **Semantic Search**: Concept-based understanding and matching
- **Cross-Modal Retrieval**: Search across different content types
- **Citation Tracking**: Source verification and trust scoring

## ğŸ“ Project Structure

```
â”œâ”€â”€ rag/                          # Advanced RAG System
â”œâ”€â”€ multi_agent_system/           # Multi-Agent Architecture
â”œâ”€â”€ observability/                # Comprehensive Monitoring System
â”œâ”€â”€ github-frontend/              # Modern React Frontend
â”œâ”€â”€ python_sdk/                   # Python Client SDK
â”œâ”€â”€ microservices/                # Microservices Architecture
â”œâ”€â”€ frontend/                     # Legacy Frontend
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ tests/                        # Test Files
â”œâ”€â”€ config/                       # Configuration Files
â”œâ”€â”€ data/                         # Data and Reports
â””â”€â”€ scripts/                      # Utility Scripts
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- Node.js 16+ (for frontend)
- SQLite 3
- Docker (optional)

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd manufacturing-knowledge-base
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install frontend dependencies**
   ```bash
   cd github-frontend
   npm install
   ```

4. **Initialize the database**
   ```bash
   python setup_models.py
   ```

5. **Start the system**
   ```bash
   # Start the main API server
   python api_server_knowledge.py --port 8001

   # Start the chat interface
   python api_chat_interface.py --port 8002

   # Start the frontend (optional)
   cd github-frontend && npm start
   ```

## ğŸ“– Usage Examples

### Basic RAG Query
```python
from rag.advanced_rag_system import create_advanced_rag_system

# Initialize RAG system
rag_system = await create_advanced_rag_system()
await rag_system.initialize()

# Query the system
response = await rag_system.query(
    "What are the safety procedures for HAAS VF-2 CNC machines?"
)

print(response.answer)
```

### Multi-Agent Orchestration
```python
from multi_agent_system import create_multi_agent_orchestrator

# Initialize agent system
orchestrator = await create_multi_agent_orchestrator()

# Process complex manufacturing query
result = await orchestrator.process_query(
    "Analyze quote trends for titanium aerospace parts"
)
```

### Observability Integration
```python
from observability import create_observability_orchestrator

# Initialize observability
observability = await create_observability_orchestrator()

# Track AI interactions
await observability.log_ai_interaction(
    session_id="session_001",
    user_id="user_123",
    query="Manufacturing safety procedures",
    response="Detailed safety guidelines...",
    performance_data={"response_time_ms": 1200}
)
```

## ğŸ”§ Configuration

### Environment Variables
```bash
# Database
DATABASE_PATH=knowledge_base.db

# AI Services (Optional)
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key

# LangFuse (Optional)
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
```

### Advanced Configuration
See `config/` directory for detailed configuration options.

## ğŸ“Š Dashboard & Monitoring

### Real-time Dashboard
- **WebSocket Connection**: `ws://localhost:8765`
- **System Health**: CPU, memory, API performance
- **Manufacturing KPIs**: Quote accuracy, quality metrics, customer satisfaction
- **User Analytics**: Behavior patterns and knowledge gaps

### Monitoring Features
- **AI Interaction Tracking**: Complete audit trail
- **Cost Analysis**: Per-operation cost breakdown
- **Performance Metrics**: Real-time system performance
- **Alert Management**: Intelligent anomaly detection
- **User Insights**: Behavior analytics and recommendations

## ğŸš€ Deployment

### Docker Deployment
```bash
# Build and start all services
docker-compose up -d
```

### Production Setup
1. Configure environment variables
2. Set up monitoring and alerting
3. Configure database backups
4. Set up SSL/TLS certificates
5. Configure load balancing

## ğŸ“š Documentation

- [API Documentation](docs/API_DESIGN.md)
- [Multi-Agent System](docs/MULTI_AGENT_SYSTEM_DOCUMENTATION.md)
- [Advanced RAG System](docs/ADVANCED_RAG_SYSTEM_DOCUMENTATION.md)
- [Observability Guide](docs/OBSERVABILITY_GUIDE.md)
- [Microservices Architecture](docs/MICROSERVICES_README.md)

## ğŸ” Manufacturing Use Cases

### Quote Management
- Automated quote generation with cost analysis
- Accuracy tracking and improvement
- Customer preference learning
- Competitive analysis integration

### Quality Control
- Document classification and processing
- Quality procedure enforcement
- Compliance tracking and reporting
- Defect analysis and prevention

### Document Processing
- Technical drawing analysis
- Specification extraction
- Cross-reference linking
- Version control management

### Customer Service
- Intelligent query routing
- Personalized response generation
- Feedback integration and analysis
- Satisfaction tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue in the GitHub repository
- Check the documentation
- Review the examples in the `examples/` directory

---

Built with â¤ï¸ for Advanced Manufacturing Knowledge Management

This system combines state-of-the-art AI technology with manufacturing domain expertise to create a comprehensive knowledge management solution.
"""

        try:
            readme_path = os.path.join(self.project_path, "README.md")
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            logger.info("âœ… Created updated README.md")
            self.cleanup_log.append("Updated README.md with comprehensive project overview")
        except Exception as e:
            logger.error(f"Failed to create updated README: {e}")

    def generate_project_stats(self):
        """ç”Ÿæˆé¡¹ç›®ç»Ÿè®¡"""
        try:
            stats = {
                "python_files": 0,
                "javascript_files": 0,
                "yaml_files": 0,
                "markdown_files": 0,
                "json_files": 0,
                "total_files": 0,
                "total_size_mb": 0,
                "directories": 0
            }

            total_size = 0

            for root, dirs, files in os.walk(self.project_path):
                # è·³è¿‡æŸäº›ç›®å½•
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

                stats["directories"] += len(dirs)

                for file in files:
                    file_path = os.path.join(root, file)

                    try:
                        if file.endswith('.py'):
                            stats["python_files"] += 1
                        elif file.endswith(('.js', '.jsx', '.ts', '.tsx')):
                            stats["javascript_files"] += 1
                        elif file.endswith(('.yaml', '.yml')):
                            stats["yaml_files"] += 1
                        elif file.endswith('.md'):
                            stats["markdown_files"] += 1
                        elif file.endswith('.json'):
                            stats["json_files"] += 1

                        stats["total_files"] += 1

                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                    except:
                        pass

            stats["total_size_mb"] = round(total_size / (1024 * 1024), 2)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_path = os.path.join(self.project_path, "data", "project_stats.json")
            os.makedirs(os.path.dirname(stats_path), exist_ok=True)

            import json
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ“Š Project Statistics:")
            logger.info(f"  Python files: {stats['python_files']}")
            logger.info(f"  JavaScript files: {stats['javascript_files']}")
            logger.info(f"  Markdown files: {stats['markdown_files']}")
            logger.info(f"  Total files: {stats['total_files']}")
            logger.info(f"  Total size: {stats['total_size_mb']} MB")

            self.cleanup_log.append(f"Project stats: {stats}")

        except Exception as e:
            logger.error(f"Failed to generate project stats: {e}")

    def create_deployment_checklist(self):
        """åˆ›å»ºéƒ¨ç½²æ¸…å•"""
        checklists = [
            "âœ… Create project backup",
            "âœ… Clean temporary files",
            "âœ… Organize project structure",
            "âœ… Update README.md",
            "âœ… Generate project statistics",
            "â³ Add all new files to Git",
            "â³ Create comprehensive commit",
            "â³ Push to GitHub repository",
            "â³ Verify deployment status"
        ]

        checklist_path = os.path.join(self.project_path, "DEPLOYMENT_CHECKLIST.md")
        with open(checklist_path, 'w', encoding='utf-8') as f:
            f.write("# Deployment Checklist\n\n")
            for item in checklists:
                f.write(f"{item}\n")

        logger.info("âœ… Created deployment checklist")
        self.cleanup_log.append("Created deployment checklist")

    def save_cleanup_log(self):
        """ä¿å­˜æ¸…ç†æ—¥å¿—"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_path = os.path.join(self.project_path, "cleanup_log.json")

            import json
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "backup_created": self.backup_created,
                "cleanup_actions": self.cleanup_log
            }

            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ’¾ Cleanup log saved to cleanup_log.json")

        except Exception as e:
            logger.error(f"Failed to save cleanup log: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ Manufacturing Knowledge Base - Project Cleanup")
    print("=" * 50)

    try:
        cleaner = ProjectCleanup()
        cleaner.run_cleanup()
        print("\nğŸ‰ Project cleanup completed successfully!")
        print("\nNext steps:")
        print("1. Review the cleanup_log.json for details")
        print("2. Check the updated project structure")
        print("3. Run: git add .")
        print("4. Run: git commit -m 'Project cleanup and organization update'")
        print("5. Run: git push origin main")

    except Exception as e:
        logger.error(f"âŒ Cleanup failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()